{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Forecast Review\n\nPulls all data sources needed for forecast review:\n1. CT metadata (fetched from API, saved for future use)\n2. VP data (user-provided local file)\n3. VOVI forecasts (US + CA, AMZL, premium)\n4. Pipeline artifacts from S3\n5. Intraday PBA data from S3"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "setup",
    "type": "python"
   },
   "source": [
    "# name: setup | type: python\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Auto-calculate target date (tomorrow Pacific)\n",
    "pacific = pytz.timezone('US/Pacific')\n",
    "now_pacific = datetime.now(pacific)\n",
    "tomorrow = now_pacific + timedelta(days=1)\n",
    "target_date = tomorrow.strftime('%Y-%m-%d')\n",
    "\n",
    "# Generate context ID\n",
    "ctx_id = generate_ctx_id('forecast_review')\n",
    "\n",
    "# Build context output directory\n",
    "ctx_dir = contexts_dir / ctx_id\n",
    "ctx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Context ID: {ctx_id}')\n",
    "print(f'Target date: {target_date}')\n",
    "print(f'VP file: {vp_file_path}')\n",
    "print(f'Output: {ctx_dir}')\n",
    "\n",
    "# Validate VP file exists\n",
    "vp_path = Path(vp_file_path)\n",
    "if not vp_path.exists():\n",
    "    raise FileNotFoundError(f'VP file not found: {vp_file_path}')\n",
    "\n",
    "result = {\n",
    "    'ctx_id': ctx_id,\n",
    "    'target_date': target_date,\n",
    "    'ctx_dir': str(ctx_dir)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "fetch_ct",
    "type": "python"
   },
   "source": [
    "# name: fetch_ct | type: python\n",
    "# Fetch CT metadata from API — saved for future use, not actively used in v1\n",
    "import json as _json\n",
    "\n",
    "ct_result_json = fetch_ct_metadata(ctx_id)\n",
    "ct_result = _json.loads(ct_result_json)\n",
    "\n",
    "if ct_result.get('success'):\n",
    "    print(f'CT metadata: {ct_result[\"row_count\"]} stations')\n",
    "    print(f'Saved to: {ct_result[\"output_file\"]}')\n",
    "else:\n",
    "    print(f'CT metadata fetch failed (non-blocking): {ct_result.get(\"error\", \"unknown\")}')\n",
    "    print('Continuing without CT metadata — not required for v1')\n",
    "\n",
    "result = ct_result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "load_vp",
    "type": "python"
   },
   "source": [
    "# name: load_vp | type: python\n",
    "# Load user-provided VP file into DuckDB\n",
    "import shutil\n",
    "\n",
    "# Copy VP file to context directory\n",
    "vp_ctx_dir = ctx_dir / 'vp'\n",
    "vp_ctx_dir.mkdir(parents=True, exist_ok=True)\n",
    "vp_dest = vp_ctx_dir / vp_path.name\n",
    "shutil.copy2(str(vp_path), str(vp_dest))\n",
    "\n",
    "# Register in DuckDB\n",
    "conn.execute(f\"CREATE OR REPLACE TABLE vp AS SELECT * FROM read_csv_auto('{vp_dest}')\")\n",
    "vp_count = conn.execute('SELECT COUNT(*) FROM vp').fetchone()[0]\n",
    "vp_cols = [col[0] for col in conn.execute('DESCRIBE vp').fetchall()]\n",
    "\n",
    "print(f'VP data: {vp_count:,} rows, {len(vp_cols)} columns')\n",
    "print(f'Columns: {vp_cols}')\n",
    "print(f'Saved to: {vp_dest}')\n",
    "\n",
    "result = {\n",
    "    'rows': vp_count,\n",
    "    'columns': vp_cols,\n",
    "    'path': str(vp_dest)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "fetch_vovi",
    "type": "python"
   },
   "source": "# name: fetch_vovi | type: python\n# Fetch VOVI forecasts for US and CA\nimport subprocess\nimport json as _json\n\ncookie_path = str(Path.home() / '.midway' / 'cookie')\nvovi_base = 'https://prod.vovi.last-mile.amazon.dev/api/forecast/list_approved'\n\ncountries = ['US', 'CA']\nbusiness_type = 'amzl'\nshipping_type = 'premium'\n\nvovi_ctx_dir = ctx_dir / 'vovi'\nvovi_ctx_dir.mkdir(parents=True, exist_ok=True)\n\nvovi_results = []\n\nfor country in countries:\n    url = f'{vovi_base}?country={country}&cptDateKey={target_date}&shippingType={shipping_type}&businessType={business_type}'\n    print(f'Fetching VOVI: {country} / {business_type} / {shipping_type} / {target_date}...')\n    \n    try:\n        curl_result = subprocess.run(\n            ['curl.exe', '--location-trusted', '-b', cookie_path, '-s', url],\n            capture_output=True, text=True\n        )\n        \n        if curl_result.returncode != 0:\n            print(f'  {country}: curl failed - {curl_result.stderr[:100]}')\n            vovi_results.append({'country': country, 'success': False, 'error': curl_result.stderr[:200]})\n            continue\n        \n        data = _json.loads(curl_result.stdout)\n        df = pd.DataFrame(data)\n        \n        # Save to context directory\n        csv_file = vovi_ctx_dir / f'vovi_{country.lower()}_{business_type}_{shipping_type}.csv'\n        df.to_csv(csv_file, index=False)\n        \n        # Register in DuckDB\n        table_name = f'vovi_{country.lower()}'\n        conn.register(table_name, df)\n        \n        print(f'  {country}: {len(df):,} rows, {len(df.columns)} cols -> {table_name}')\n        vovi_results.append({'country': country, 'success': True, 'rows': len(df), 'table': table_name, 'path': str(csv_file)})\n        \n    except Exception as e:\n        print(f'  {country}: failed - {e}')\n        vovi_results.append({'country': country, 'success': False, 'error': str(e)})\n\n# Create combined vovi table using UNION ALL BY NAME (handles differing column counts)\ntry:\n    tables_to_union = [r['table'] for r in vovi_results if r.get('success')]\n    if tables_to_union:\n        union_sql = ' UNION ALL BY NAME '.join([f\"SELECT * FROM {t}\" for t in tables_to_union])\n        conn.execute(f'CREATE OR REPLACE VIEW vovi AS {union_sql}')\n        vovi_total = conn.execute('SELECT COUNT(*) FROM vovi').fetchone()[0]\n        print(f'\\nCombined vovi view: {vovi_total:,} rows')\nexcept Exception as e:\n    print(f'Could not create combined view: {e}')\n\nresult = vovi_results",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "download_artifacts",
    "type": "python"
   },
   "source": "# name: download_artifacts | type: python\n# Download latest pipeline artifacts from S3\nimport re\n\nartifact_bucket = 'lma-glue-pipeline'\nsort_code = 'DS-A'\nartifacts_dir = ctx_dir / 'pipeline_artifacts'\nartifacts_dir.mkdir(parents=True, exist_ok=True)\n\nsession = make_boto3_session(profile_name=aws_profile)\ns3 = session.client('s3')\n\ns3_prefix = f'pipeline_output/internal_sort_code={sort_code}/target_forecast_date={target_date}/'\nprint(f'Scanning: s3://{artifact_bucket}/{s3_prefix}')\n\n# Group by artifact type, keep latest timestamp\npaginator = s3.get_paginator('list_objects_v2')\nfiles_by_type = {}\n\nfor page in paginator.paginate(Bucket=artifact_bucket, Prefix=s3_prefix):\n    for obj in page.get('Contents', []):\n        key = obj['Key']\n        filename = key.split('/')[-1]\n        match = re.match(r'^(.+?)_(\\d{8}_\\d{6})(.*)\\.csv$', filename)\n        if match:\n            base_name = match.group(1)\n            timestamp = match.group(2)\n            suffix = match.group(3)\n            artifact_type = base_name + suffix\n            if artifact_type not in files_by_type or timestamp > files_by_type[artifact_type]['timestamp']:\n                files_by_type[artifact_type] = {\n                    'key': key, 'filename': filename, 'timestamp': timestamp,\n                    'size': obj['Size'], 'artifact_type': artifact_type\n                }\n\nprint(f'Found {len(files_by_type)} artifact types')\n\n# Download and register each\nartifact_results = []\nfor atype, info in sorted(files_by_type.items()):\n    dest_file = artifacts_dir / info['filename']\n    size_mb = info['size'] / 1024 / 1024\n    \n    try:\n        print(f'  {info[\"filename\"]} ({size_mb:.1f} MB)...', flush=True)\n        s3.download_file(artifact_bucket, info['key'], str(dest_file))\n        \n        # Register in DuckDB\n        conn.execute(f\"CREATE OR REPLACE TABLE {atype} AS SELECT * FROM read_csv_auto('{dest_file}')\")\n        row_count = conn.execute(f'SELECT COUNT(*) FROM {atype}').fetchone()[0]\n        \n        artifact_results.append({'artifact': atype, 'rows': row_count, 'size_mb': round(size_mb, 1), 'path': str(dest_file)})\n    except Exception as e:\n        print(f'    FAILED: {e}')\n        artifact_results.append({'artifact': atype, 'error': str(e)})\n\nprint(f'\\nDownloaded and registered {len([a for a in artifact_results if \"rows\" in a])}/{len(files_by_type)} artifacts')\n\nresult = artifact_results",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# name: download_pba | type: python\n# Download latest intraday PBA parquet from S3 (last-mile-staging bucket)\nimport re as _re\n\npba_bucket = 'last-mile-staging'\nsort_code = 'DS-A'\npba_dir = ctx_dir / 'intraday_pba'\npba_dir.mkdir(parents=True, exist_ok=True)\n\npba_session = make_boto3_session(profile_name='last-mile-staging')\npba_s3 = pba_session.client('s3')\n\npba_prefix = f'intraday-pba/partition_internal_sort_code={sort_code}/partition_target_date={target_date}/'\nprint(f'Scanning: s3://{pba_bucket}/{pba_prefix}')\n\n# List all objects under target date to find run_time partitions\npaginator = pba_s3.get_paginator('list_objects_v2')\nrun_times = {}\n\nfor page in paginator.paginate(Bucket=pba_bucket, Prefix=pba_prefix):\n    for obj in page.get('Contents', []):\n        key = obj['Key']\n        match = _re.search(r'partition_run_time=(\\d+)/', key)\n        if match:\n            run_time = int(match.group(1))\n            run_times[run_time] = {'key': key, 'size': obj['Size']}\n\nif not run_times:\n    print(f'No intraday PBA data found for {target_date}')\n    result = {'success': False, 'error': f'No data for {target_date}'}\nelse:\n    latest_run_time = max(run_times)\n    latest = run_times[latest_run_time]\n    filename = latest['key'].split('/')[-1]\n    size_mb = latest['size'] / 1024 / 1024\n    dest_file = pba_dir / filename\n\n    print(f'Latest run_time: {latest_run_time}')\n    print(f'Downloading: {filename} ({size_mb:.1f} MB)...', flush=True)\n\n    pba_s3.download_file(pba_bucket, latest['key'], str(dest_file))\n\n    # Register in DuckDB\n    conn.execute(f\"CREATE OR REPLACE TABLE intraday_pba AS SELECT * FROM read_parquet('{dest_file}')\")\n    pba_count = conn.execute('SELECT COUNT(*) FROM intraday_pba').fetchone()[0]\n    pba_cols = [col[0] for col in conn.execute('DESCRIBE intraday_pba').fetchall()]\n\n    print(f'Intraday PBA: {pba_count:,} rows, {len(pba_cols)} columns')\n    print(f'Columns: {pba_cols}')\n    print(f'Saved to: {dest_file}')\n\n    result = {\n        'success': True,\n        'run_time': latest_run_time,\n        'rows': pba_count,\n        'columns': pba_cols,\n        'size_mb': round(size_mb, 1),\n        'path': str(dest_file)\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "summary",
    "type": "python"
   },
   "source": [
    "# name: summary | type: python\n",
    "# Print summary of all loaded data\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Forecast Review Context: {ctx_id}')\n",
    "print(f'Target Date: {target_date}')\n",
    "print(f'Output Directory: {ctx_dir}')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "# List all registered tables\n",
    "tables = conn.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'\").fetchall()\n",
    "print(f'\\nRegistered tables ({len(tables)}):')\n",
    "for t in tables:\n",
    "    name = t[0]\n",
    "    try:\n",
    "        count = conn.execute(f'SELECT COUNT(*) FROM \"{name}\"').fetchone()[0]\n",
    "        print(f'  {name}: {count:,} rows')\n",
    "    except:\n",
    "        print(f'  {name}: (view)')\n",
    "\n",
    "result = {\n",
    "    'status': 'success',\n",
    "    'ctx_id': ctx_id,\n",
    "    'target_date': target_date,\n",
    "    'ctx_dir': str(ctx_dir),\n",
    "    'tables': [t[0] for t in tables]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "etl": {
   "name": "Forecast Review",
   "description": "Pulls VP, VOVI, CT metadata, and pipeline artifacts for forecast review",
   "variables": {
    "vp_file_path": {
     "type": "text",
     "label": "VP File Path",
     "default": ""
    },
    "aws_profile": {
     "type": "text",
     "label": "AWS Profile for S3",
     "default": "forecast-artifacts"
    }
   },
   "sources": [],
   "outputs": [],
   "options": {
    "return_preview": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}