{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Forecast Setup\n\nData pulls, then transforms.\n\nInputs (injected variables):\n1. `site_list_path` — Site list Excel file\n2. `ct_file_path` — VP/CT CSV file\n\nData pulls:\n3. VOVI forecasts (US + CA, AMZL, premium)\n4. Pipeline artifacts from S3\n5. Intraday PBA data from S3\n\nTransforms:\n- VP pivot (long to wide, util columns, grid keys)\n- Site list + VP + VOVI outer join with available_inputs flag\n- PBA query (separate output file)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "setup",
    "type": "python"
   },
   "source": "# name: setup | type: python\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport pytz\n\nsite_list = Path(site_list_path)\nct_file = Path(ct_file_path)\n\nif not site_list.exists():\n    raise FileNotFoundError(f'Site list not found: {site_list_path}')\nif not ct_file.exists():\n    raise FileNotFoundError(f'CT file not found: {ct_file_path}')\n\n# Auto-calculate target date (tomorrow Pacific)\npacific = pytz.timezone('US/Pacific')\nnow_pacific = datetime.now(pacific)\ntomorrow = now_pacific + timedelta(days=1)\ntarget_date = tomorrow.strftime('%Y-%m-%d')\n\n# Generate context ID and output directory\nctx_id = generate_ctx_id('forecast_setup')\nctx_dir = contexts_dir / ctx_id\nctx_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f'Site list: {site_list}')\nprint(f'CT file: {ct_file}')\nprint(f'Target date: {target_date}')\nprint(f'Context ID: {ctx_id}')\nprint(f'Output: {ctx_dir}')\n\nresult = {'site_list_path': str(site_list), 'ct_file_path': str(ct_file), 'target_date': target_date, 'ctx_id': ctx_id, 'ctx_dir': str(ctx_dir)}",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "load_site_list",
    "type": "sql"
   },
   "source": "-- name: load_site_list | type: sql\nCREATE OR REPLACE TABLE site_list AS\nSELECT\n    \"Station\" AS station,\n    \"Cycle\" AS cycle,\n    \"Business Org\" AS business_org\nFROM read_xlsx(site_list_path)\nWHERE \"Business Org\" = 'AMZL'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "load_vp",
    "type": "sql"
   },
   "source": "-- name: load_vp | type: sql\nCREATE OR REPLACE TABLE vp_raw AS\nSELECT * FROM read_csv_auto(ct_file_path)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# name: fetch_vovi | type: python\n# Fetch VOVI forecasts for US and CA\nimport subprocess\nimport json as _json\n\ncookie_path = str(Path.home() / '.midway' / 'cookie')\nvovi_base = 'https://prod.vovi.last-mile.amazon.dev/api/forecast/list_approved'\n\ncountries = ['US', 'CA']\nbusiness_type = 'amzl'\nshipping_type = 'premium'\n\nvovi_ctx_dir = ctx_dir / 'vovi'\nvovi_ctx_dir.mkdir(parents=True, exist_ok=True)\n\nvovi_results = []\n\nfor country in countries:\n    url = f'{vovi_base}?country={country}&cptDateKey={target_date}&shippingType={shipping_type}&businessType={business_type}'\n    print(f'Fetching VOVI: {country} / {business_type} / {shipping_type} / {target_date}...')\n    \n    try:\n        curl_result = subprocess.run(\n            ['curl.exe', '--location-trusted', '-b', cookie_path, '-s', url],\n            capture_output=True, text=True\n        )\n        \n        if curl_result.returncode != 0:\n            print(f'  {country}: curl failed - {curl_result.stderr[:100]}')\n            vovi_results.append({'country': country, 'success': False, 'error': curl_result.stderr[:200]})\n            continue\n        \n        data = _json.loads(curl_result.stdout)\n        df = pd.DataFrame(data)\n        \n        # Save to context directory\n        csv_file = vovi_ctx_dir / f'vovi_{country.lower()}_{business_type}_{shipping_type}.csv'\n        df.to_csv(csv_file, index=False)\n        \n        # Register in DuckDB\n        table_name = f'vovi_{country.lower()}'\n        conn.register(table_name, df)\n        \n        print(f'  {country}: {len(df):,} rows, {len(df.columns)} cols -> {table_name}')\n        vovi_results.append({'country': country, 'success': True, 'rows': len(df), 'table': table_name, 'path': str(csv_file)})\n        \n    except Exception as e:\n        print(f'  {country}: failed - {e}')\n        vovi_results.append({'country': country, 'success': False, 'error': str(e)})\n\n# Create combined vovi table using UNION ALL BY NAME (handles differing column counts)\ntry:\n    tables_to_union = [r['table'] for r in vovi_results if r.get('success')]\n    if tables_to_union:\n        union_sql = ' UNION ALL BY NAME '.join([f\"SELECT * FROM {t}\" for t in tables_to_union])\n        conn.execute(f'CREATE OR REPLACE VIEW vovi AS {union_sql}')\n        vovi_total = conn.execute('SELECT COUNT(*) FROM vovi').fetchone()[0]\n        print(f'\\nCombined vovi view: {vovi_total:,} rows')\nexcept Exception as e:\n    print(f'Could not create combined view: {e}')\n\nresult = vovi_results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# name: download_artifacts | type: python\n# Download latest pipeline artifacts from S3\nimport re\n\nartifact_bucket = 'lma-glue-pipeline'\nsort_code = 'DS-A'\nartifacts_dir = ctx_dir / 'pipeline_artifacts'\nartifacts_dir.mkdir(parents=True, exist_ok=True)\n\nsession = make_boto3_session(profile_name=aws_profile)\ns3 = session.client('s3')\n\ns3_prefix = f'pipeline_output/internal_sort_code={sort_code}/target_forecast_date={target_date}/'\nprint(f'Scanning: s3://{artifact_bucket}/{s3_prefix}')\n\n# Group by artifact type, keep latest timestamp\npaginator = s3.get_paginator('list_objects_v2')\nfiles_by_type = {}\n\nfor page in paginator.paginate(Bucket=artifact_bucket, Prefix=s3_prefix):\n    for obj in page.get('Contents', []):\n        key = obj['Key']\n        filename = key.split('/')[-1]\n        match = re.match(r'^(.+?)_(\\d{8}_\\d{6})(.*)\\.csv$', filename)\n        if match:\n            base_name = match.group(1)\n            timestamp = match.group(2)\n            suffix = match.group(3)\n            artifact_type = base_name + suffix\n            if artifact_type not in files_by_type or timestamp > files_by_type[artifact_type]['timestamp']:\n                files_by_type[artifact_type] = {\n                    'key': key, 'filename': filename, 'timestamp': timestamp,\n                    'size': obj['Size'], 'artifact_type': artifact_type\n                }\n\nprint(f'Found {len(files_by_type)} artifact types')\n\n# Download and register each\nartifact_results = []\nfor atype, info in sorted(files_by_type.items()):\n    dest_file = artifacts_dir / info['filename']\n    size_mb = info['size'] / 1024 / 1024\n    \n    try:\n        print(f'  {info[\"filename\"]} ({size_mb:.1f} MB)...', flush=True)\n        s3.download_file(artifact_bucket, info['key'], str(dest_file))\n        \n        # Register in DuckDB\n        conn.execute(f\"CREATE OR REPLACE TABLE {atype} AS SELECT * FROM read_csv_auto('{dest_file}')\")\n        row_count = conn.execute(f'SELECT COUNT(*) FROM {atype}').fetchone()[0]\n        \n        artifact_results.append({'artifact': atype, 'rows': row_count, 'size_mb': round(size_mb, 1), 'path': str(dest_file)})\n    except Exception as e:\n        print(f'    FAILED: {e}')\n        artifact_results.append({'artifact': atype, 'error': str(e)})\n\nprint(f'\\nDownloaded and registered {len([a for a in artifact_results if \"rows\" in a])}/{len(files_by_type)} artifacts')\n\nresult = artifact_results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# name: download_pba | type: python\n# Download latest intraday PBA parquet from S3 (last-mile-staging bucket)\nimport re as _re\n\npba_bucket = 'last-mile-staging'\nsort_code = 'DS-A'\npba_dir = ctx_dir / 'intraday_pba'\npba_dir.mkdir(parents=True, exist_ok=True)\n\npba_session = make_boto3_session(profile_name='last-mile-staging')\npba_s3 = pba_session.client('s3')\n\npba_prefix = f'intraday-pba/partition_internal_sort_code={sort_code}/partition_target_date={target_date}/'\nprint(f'Scanning: s3://{pba_bucket}/{pba_prefix}')\n\n# List all objects under target date to find run_time partitions\npaginator = pba_s3.get_paginator('list_objects_v2')\nrun_times = {}\n\nfor page in paginator.paginate(Bucket=pba_bucket, Prefix=pba_prefix):\n    for obj in page.get('Contents', []):\n        key = obj['Key']\n        match = _re.search(r'partition_run_time=(\\d+)/', key)\n        if match:\n            run_time = int(match.group(1))\n            run_times[run_time] = {'key': key, 'size': obj['Size']}\n\nif not run_times:\n    print(f'No intraday PBA data found for {target_date}')\n    result = {'success': False, 'error': f'No data for {target_date}'}\nelse:\n    latest_run_time = max(run_times)\n    latest = run_times[latest_run_time]\n    filename = latest['key'].split('/')[-1]\n    size_mb = latest['size'] / 1024 / 1024\n    dest_file = pba_dir / filename\n\n    print(f'Latest run_time: {latest_run_time}')\n    print(f'Downloading: {filename} ({size_mb:.1f} MB)...', flush=True)\n\n    pba_s3.download_file(pba_bucket, latest['key'], str(dest_file))\n\n    # Register in DuckDB\n    conn.execute(f\"CREATE OR REPLACE TABLE intraday_pba AS SELECT * FROM read_parquet('{dest_file}')\")\n    pba_count = conn.execute('SELECT COUNT(*) FROM intraday_pba').fetchone()[0]\n    pba_cols = [col[0] for col in conn.execute('DESCRIBE intraday_pba').fetchall()]\n\n    print(f'Intraday PBA: {pba_count:,} rows, {len(pba_cols)} columns')\n    print(f'Columns: {pba_cols}')\n    print(f'Saved to: {dest_file}')\n\n    result = {\n        'success': True,\n        'run_time': latest_run_time,\n        'rows': pba_count,\n        'columns': pba_cols,\n        'size_mb': round(size_mb, 1),\n        'path': str(dest_file)\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "-- name: vp_pivot | type: sql\nSET TimeZone = 'UTC';\n\nCREATE OR REPLACE TABLE vp AS\nWITH pivot_all AS (\n    SELECT\n        node,\n        plan_start_date,\n        ofd_dates,\n        demand_types,\n        CAST(cpts AS TIMESTAMP) AS cpts,\n        MAX(CASE WHEN metric_name = 'total_volume_available' THEN metric_value END) AS total_volume_available,\n        MAX(CASE WHEN metric_name = 'automated_uncapped_slam_forecast' THEN metric_value END) AS automated_uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'current_slam' THEN metric_value END) AS current_slam,\n        MAX(CASE WHEN metric_name = 'weekly_uncapped_slam_forecast' THEN metric_value END) AS weekly_uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'post_cutoff_adjustment' THEN metric_value END) AS post_cutoff_adjustment,\n        MAX(CASE WHEN metric_name = 'total_backlog' THEN metric_value END) AS total_backlog,\n        MAX(CASE WHEN metric_name = 'automated_confidence' THEN metric_value END) AS automated_confidence,\n        MAX(CASE WHEN metric_name = 'uncapped_slam_forecast' THEN metric_value END) AS uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'atrops_soft_cap' THEN metric_value END) AS atrops_soft_cap,\n        MAX(CASE WHEN metric_name = 'confidence_anomaly' THEN metric_value END) AS confidence_anomaly,\n        MAX(CASE WHEN metric_name = 'net_volume_adjustments' THEN metric_value END) AS net_volume_adjustments,\n        MAX(CASE WHEN metric_name = 'adjusted_uncapped_slam_forecast' THEN metric_value END) AS adjusted_uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'cap_target_buffer' THEN metric_value END) AS cap_target_buffer,\n        MAX(CASE WHEN metric_name = 'earlies_expected' THEN metric_value END) AS earlies_expected,\n        MAX(CASE WHEN metric_name = 'returns' THEN metric_value END) AS returns,\n        MAX(CASE WHEN metric_name = 'sideline_in' THEN metric_value END) AS sideline_in,\n        MAX(CASE WHEN metric_name = 'vovi_uncapped_slam_forecast' THEN metric_value END) AS vovi_uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'in_station_backlog' THEN metric_value END) AS in_station_backlog,\n        MAX(CASE WHEN metric_name = 'mnr_expected' THEN metric_value END) AS mnr_expected,\n        MAX(CASE WHEN metric_name = 'mnr_received' THEN metric_value END) AS mnr_received,\n        MAX(CASE WHEN metric_name = 'current_schedule' THEN metric_value END) AS current_schedule,\n        MAX(CASE WHEN metric_name = 'vovi_adjustment' THEN metric_value END) AS vovi_adjustment,\n        MAX(CASE WHEN metric_name = 'forecast_type' THEN metric_value END) AS forecast_type,\n        MAX(CASE WHEN metric_name = 'earlies_received' THEN metric_value END) AS earlies_received,\n        MAX(CASE WHEN metric_name = 'latest_deployed_cap' THEN metric_value END) AS latest_deployed_cap,\n        MAX(CASE WHEN metric_name = 'atrops_hard_cap' THEN metric_value END) AS atrops_hard_cap,\n        MAX(CASE WHEN metric_name = 'capped_slam_forecast' THEN metric_value END) AS capped_slam_forecast\n    FROM vp_raw\n    WHERE plan_start_date::DATE = ofd_dates::DATE\n    GROUP BY node, plan_start_date, ofd_dates, demand_types, cpts\n)\nSELECT\n    *,\n    strftime(cpts, '%H:%M') AS cpts_local,\n    strftime(cpts AT TIME ZONE 'UTC', '%H:%M') AS cpts_utc,\n    ofd_dates || '#' || node || '#' || strftime(cpts, '%H:%M') AS grid_key_local,\n    ofd_dates || '#' || node || '#' || strftime(cpts AT TIME ZONE 'UTC', '%H:%M') AS grid_key_utc,\n    CASE WHEN GREATEST(COALESCE(latest_deployed_cap::FLOAT, 0), COALESCE(atrops_soft_cap::FLOAT, 0)) > 0\n         THEN ROUND(COALESCE(automated_uncapped_slam_forecast::FLOAT, 0) / GREATEST(COALESCE(latest_deployed_cap::FLOAT, 0), COALESCE(atrops_soft_cap::FLOAT, 0)), 4)\n         ELSE NULL END AS auto_forecast_util,\n    CASE WHEN GREATEST(COALESCE(latest_deployed_cap::FLOAT, 0), COALESCE(atrops_soft_cap::FLOAT, 0)) > 0\n         THEN ROUND(COALESCE(current_schedule::FLOAT, 0) / GREATEST(COALESCE(latest_deployed_cap::FLOAT, 0), COALESCE(atrops_soft_cap::FLOAT, 0)), 4)\n         ELSE NULL END AS util\nFROM pivot_all\nORDER BY node, cpts",
   "metadata": {
    "name": "vp_pivot",
    "type": "sql"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "-- name: joined | type: sql\nSET TimeZone = 'UTC';\n\nCREATE OR REPLACE TABLE joined AS\nSELECT\n    COALESCE(sl.station, vp.node) AS station,\n    sl.cycle,\n    sl.business_org,\n    CASE\n        WHEN sl.station IS NOT NULL AND vp.node IS NOT NULL THEN 'vp_list'\n        WHEN vp.node IS NOT NULL THEN 'vp'\n        ELSE 'list'\n    END AS available_inputs,\n    vp.plan_start_date,\n    vp.ofd_dates,\n    vp.demand_types,\n    vp.cpts,\n    vp.cpts_local,\n    vp.cpts_utc,\n    vp.grid_key_local,\n    vp.grid_key_utc,\n    vp.forecast_type,\n    vp.automated_confidence,\n    vp.auto_forecast_util,\n    vp.util,\n    vp.vovi_uncapped_slam_forecast,\n    vp.uncapped_slam_forecast,\n    vp.adjusted_uncapped_slam_forecast,\n    vp.capped_slam_forecast,\n    vp.atrops_soft_cap,\n    vp.atrops_hard_cap,\n    vp.latest_deployed_cap,\n    vp.cap_target_buffer,\n    vp.current_slam,\n    vp.current_schedule,\n    vp.total_volume_available,\n    vp.total_backlog,\n    vp.in_station_backlog,\n    vp.post_cutoff_adjustment,\n    vp.net_volume_adjustments,\n    vp.vovi_adjustment,\n    vp.confidence_anomaly,\n    vp.automated_uncapped_slam_forecast,\n    vp.weekly_uncapped_slam_forecast,\n    vp.earlies_expected,\n    vp.earlies_received,\n    vp.returns,\n    vp.sideline_in,\n    vp.mnr_expected,\n    vp.mnr_received,\n    v.modified_user AS vovi_modified_user,\n    v.proposed_cap AS vovi_proposed_cap,\n    v.post_cutoff_adjustment AS vovi_post_cutoff_adjustment,\n    v.adjusted_forecast AS vovi_adjusted_forecast,\n    v.forecast_source AS vovi_forecast_source,\n    v.original_forecast AS vovi_original_forecast,\n    v.forecast_status AS vovi_forecast_status,\n    v.forecast_adjustment AS vovi_forecast_adjustment,\n    v.current_slammed AS vovi_current_slammed,\n    v.current_scheduled AS vovi_current_scheduled,\n    v.soft_cap AS vovi_soft_cap,\n    v.hard_cap AS vovi_hard_cap,\n    NOW()::TIMESTAMP AS execution_ts\nFROM site_list sl\nFULL OUTER JOIN vp ON sl.station = vp.node\nLEFT JOIN vovi v\n    ON COALESCE(sl.station, vp.node) = v.station\n    AND vp.cpts_utc = strftime(CAST(timezone('UTC', to_timestamp(v.station_cpt)) AS TIMESTAMP), '%H:%M')\nORDER BY station, cpts",
   "metadata": {
    "name": "joined",
    "type": "sql"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# name: summary | type: python\n# Summary of available_inputs breakdown\nsummary = conn.execute(\"\"\"\n    SELECT available_inputs, COUNT(DISTINCT station) AS stations, COUNT(*) AS rows\n    FROM joined\n    GROUP BY available_inputs\n    ORDER BY available_inputs\n\"\"\").fetchdf()\nprint(summary.to_string(index=False))\n\ntotal = conn.execute(\"SELECT COUNT(*) FROM joined\").fetchone()[0]\ndistinct = conn.execute(\"SELECT COUNT(DISTINCT station) FROM joined\").fetchone()[0]\nprint(f'\\nTotal: {distinct} stations, {total} rows')\n\nresult = {\n    'status': 'success',\n    'total_rows': total,\n    'distinct_stations': distinct,\n    'breakdown': summary.to_dict('records')\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# name: save_joined | type: python\n# Export joined table to CSV\noutput_file = ctx_dir / 'joined.csv'\nconn.execute(f\"COPY joined TO '{output_file}' (HEADER, DELIMITER ',')\")\nrow_count = conn.execute(\"SELECT COUNT(*) FROM joined\").fetchone()[0]\nprint(f'Saved joined table: {row_count} rows -> {output_file}')\n\nresult = {'output_file': str(output_file), 'rows': row_count}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "etl": {
   "name": "Forecast Reviewer Transform",
   "description": "Loads site list and VP data, pivots, joins, then downloads VOVI/artifacts/PBA",
   "variables": {
    "site_list_path": {
     "type": "text",
     "label": "Site List Excel Path",
     "default": ""
    },
    "ct_file_path": {
     "type": "text",
     "label": "CT/VP CSV File Path",
     "default": ""
    },
    "aws_profile": {
     "type": "text",
     "label": "AWS Profile for S3",
     "default": "forecast-artifacts"
    }
   },
   "sources": [],
   "outputs": [],
   "options": {
    "return_preview": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}