{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Review\n",
    "\n",
    "Pulls all data sources needed for forecast review:\n",
    "1. CT metadata (fetched from API, saved for future use)\n",
    "2. VP data (user-provided local file)\n",
    "3. VOVI forecasts (US + CA, AMZL, premium)\n",
    "4. Pipeline artifacts from S3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "setup",
    "type": "python"
   },
   "source": [
    "# name: setup | type: python\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Auto-calculate target date (tomorrow Pacific)\n",
    "pacific = pytz.timezone('US/Pacific')\n",
    "now_pacific = datetime.now(pacific)\n",
    "tomorrow = now_pacific + timedelta(days=1)\n",
    "target_date = tomorrow.strftime('%Y-%m-%d')\n",
    "\n",
    "# Generate context ID\n",
    "ctx_id = generate_ctx_id('forecast_review')\n",
    "\n",
    "# Build context output directory\n",
    "ctx_dir = contexts_dir / ctx_id\n",
    "ctx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Context ID: {ctx_id}')\n",
    "print(f'Target date: {target_date}')\n",
    "print(f'VP file: {vp_file_path}')\n",
    "print(f'Output: {ctx_dir}')\n",
    "\n",
    "# Validate VP file exists\n",
    "vp_path = Path(vp_file_path)\n",
    "if not vp_path.exists():\n",
    "    raise FileNotFoundError(f'VP file not found: {vp_file_path}')\n",
    "\n",
    "result = {\n",
    "    'ctx_id': ctx_id,\n",
    "    'target_date': target_date,\n",
    "    'ctx_dir': str(ctx_dir)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "fetch_ct",
    "type": "python"
   },
   "source": [
    "# name: fetch_ct | type: python\n",
    "# Fetch CT metadata from API — saved for future use, not actively used in v1\n",
    "import json as _json\n",
    "\n",
    "ct_result_json = fetch_ct_metadata(ctx_id)\n",
    "ct_result = _json.loads(ct_result_json)\n",
    "\n",
    "if ct_result.get('success'):\n",
    "    print(f'CT metadata: {ct_result[\"row_count\"]} stations')\n",
    "    print(f'Saved to: {ct_result[\"output_file\"]}')\n",
    "else:\n",
    "    print(f'CT metadata fetch failed (non-blocking): {ct_result.get(\"error\", \"unknown\")}')\n",
    "    print('Continuing without CT metadata — not required for v1')\n",
    "\n",
    "result = ct_result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "load_vp",
    "type": "python"
   },
   "source": [
    "# name: load_vp | type: python\n",
    "# Load user-provided VP file into DuckDB\n",
    "import shutil\n",
    "\n",
    "# Copy VP file to context directory\n",
    "vp_ctx_dir = ctx_dir / 'vp'\n",
    "vp_ctx_dir.mkdir(parents=True, exist_ok=True)\n",
    "vp_dest = vp_ctx_dir / vp_path.name\n",
    "shutil.copy2(str(vp_path), str(vp_dest))\n",
    "\n",
    "# Register in DuckDB\n",
    "conn.execute(f\"CREATE OR REPLACE TABLE vp AS SELECT * FROM read_csv_auto('{vp_dest}')\")\n",
    "vp_count = conn.execute('SELECT COUNT(*) FROM vp').fetchone()[0]\n",
    "vp_cols = [col[0] for col in conn.execute('DESCRIBE vp').fetchall()]\n",
    "\n",
    "print(f'VP data: {vp_count:,} rows, {len(vp_cols)} columns')\n",
    "print(f'Columns: {vp_cols}')\n",
    "print(f'Saved to: {vp_dest}')\n",
    "\n",
    "result = {\n",
    "    'rows': vp_count,\n",
    "    'columns': vp_cols,\n",
    "    'path': str(vp_dest)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "fetch_vovi",
    "type": "python"
   },
   "source": [
    "# name: fetch_vovi | type: python\n",
    "# Fetch VOVI forecasts for US and CA\n",
    "import subprocess\n",
    "import json as _json\n",
    "\n",
    "cookie_path = str(Path.home() / '.midway' / 'cookie')\n",
    "vovi_base = 'https://prod.vovi.last-mile.amazon.dev/api/forecast/list_approved'\n",
    "\n",
    "countries = ['US', 'CA']\n",
    "business_type = 'amzl'\n",
    "shipping_type = 'premium'\n",
    "\n",
    "vovi_ctx_dir = ctx_dir / 'vovi'\n",
    "vovi_ctx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vovi_results = []\n",
    "\n",
    "for country in countries:\n",
    "    url = f'{vovi_base}?country={country}&cptDateKey={target_date}&shippingType={shipping_type}&businessType={business_type}'\n",
    "    print(f'Fetching VOVI: {country} / {business_type} / {shipping_type} / {target_date}...')\n",
    "    \n",
    "    try:\n",
    "        curl_result = subprocess.run(\n",
    "            ['curl.exe', '--location-trusted', '-b', cookie_path, '-s', url],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        \n",
    "        if curl_result.returncode != 0:\n",
    "            print(f'  {country}: curl failed - {curl_result.stderr[:100]}')\n",
    "            vovi_results.append({'country': country, 'success': False, 'error': curl_result.stderr[:200]})\n",
    "            continue\n",
    "        \n",
    "        data = _json.loads(curl_result.stdout)\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Save to context directory\n",
    "        csv_file = vovi_ctx_dir / f'vovi_{country.lower()}_{business_type}_{shipping_type}.csv'\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        # Register in DuckDB\n",
    "        table_name = f'vovi_{country.lower()}'\n",
    "        conn.register(table_name, df)\n",
    "        \n",
    "        print(f'  {country}: {len(df):,} rows -> {table_name}')\n",
    "        vovi_results.append({'country': country, 'success': True, 'rows': len(df), 'table': table_name, 'path': str(csv_file)})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'  {country}: failed - {e}')\n",
    "        vovi_results.append({'country': country, 'success': False, 'error': str(e)})\n",
    "\n",
    "# Also create a combined vovi table\n",
    "try:\n",
    "    tables_to_union = [r['table'] for r in vovi_results if r.get('success')]\n",
    "    if tables_to_union:\n",
    "        union_sql = ' UNION ALL '.join([f\"SELECT *, '{t.split('_')[1].upper()}' as country FROM {t}\" for t in tables_to_union])\n",
    "        conn.execute(f'CREATE OR REPLACE VIEW vovi AS {union_sql}')\n",
    "        vovi_total = conn.execute('SELECT COUNT(*) FROM vovi').fetchone()[0]\n",
    "        print(f'\\nCombined vovi view: {vovi_total:,} rows')\n",
    "except Exception as e:\n",
    "    print(f'Could not create combined view: {e}')\n",
    "\n",
    "result = vovi_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "download_artifacts",
    "type": "python"
   },
   "source": [
    "# name: download_artifacts | type: python\n",
    "# Download latest pipeline artifacts from S3\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "artifact_bucket = 'lma-glue-pipeline'\n",
    "sort_code = 'DS-A'\n",
    "artifacts_dir = ctx_dir / 'pipeline_artifacts'\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "session = boto3.Session(profile_name=aws_profile)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "s3_prefix = f'pipeline_output/internal_sort_code={sort_code}/target_forecast_date={target_date}/'\n",
    "print(f'Scanning: s3://{artifact_bucket}/{s3_prefix}')\n",
    "\n",
    "# Group by artifact type, keep latest timestamp\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "files_by_type = {}\n",
    "\n",
    "for page in paginator.paginate(Bucket=artifact_bucket, Prefix=s3_prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        filename = key.split('/')[-1]\n",
    "        match = re.match(r'^(.+?)_(\\d{8}_\\d{6})(.*)\\.csv$', filename)\n",
    "        if match:\n",
    "            base_name = match.group(1)\n",
    "            timestamp = match.group(2)\n",
    "            suffix = match.group(3)\n",
    "            artifact_type = base_name + suffix\n",
    "            if artifact_type not in files_by_type or timestamp > files_by_type[artifact_type]['timestamp']:\n",
    "                files_by_type[artifact_type] = {\n",
    "                    'key': key, 'filename': filename, 'timestamp': timestamp,\n",
    "                    'size': obj['Size'], 'artifact_type': artifact_type\n",
    "                }\n",
    "\n",
    "print(f'Found {len(files_by_type)} artifact types')\n",
    "\n",
    "# Download and register each\n",
    "artifact_results = []\n",
    "for atype, info in sorted(files_by_type.items()):\n",
    "    dest_file = artifacts_dir / info['filename']\n",
    "    size_mb = info['size'] / 1024 / 1024\n",
    "    \n",
    "    try:\n",
    "        print(f'  {info[\"filename\"]} ({size_mb:.1f} MB)...', flush=True)\n",
    "        s3.download_file(artifact_bucket, info['key'], str(dest_file))\n",
    "        \n",
    "        # Register in DuckDB\n",
    "        conn.execute(f\"CREATE OR REPLACE TABLE {atype} AS SELECT * FROM read_csv_auto('{dest_file}')\")\n",
    "        row_count = conn.execute(f'SELECT COUNT(*) FROM {atype}').fetchone()[0]\n",
    "        \n",
    "        artifact_results.append({'artifact': atype, 'rows': row_count, 'size_mb': round(size_mb, 1), 'path': str(dest_file)})\n",
    "    except Exception as e:\n",
    "        print(f'    FAILED: {e}')\n",
    "        artifact_results.append({'artifact': atype, 'error': str(e)})\n",
    "\n",
    "print(f'\\nDownloaded and registered {len([a for a in artifact_results if \"rows\" in a])}/{len(files_by_type)} artifacts')\n",
    "\n",
    "result = artifact_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "summary",
    "type": "python"
   },
   "source": [
    "# name: summary | type: python\n",
    "# Print summary of all loaded data\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Forecast Review Context: {ctx_id}')\n",
    "print(f'Target Date: {target_date}')\n",
    "print(f'Output Directory: {ctx_dir}')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "# List all registered tables\n",
    "tables = conn.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'\").fetchall()\n",
    "print(f'\\nRegistered tables ({len(tables)}):')\n",
    "for t in tables:\n",
    "    name = t[0]\n",
    "    try:\n",
    "        count = conn.execute(f'SELECT COUNT(*) FROM \"{name}\"').fetchone()[0]\n",
    "        print(f'  {name}: {count:,} rows')\n",
    "    except:\n",
    "        print(f'  {name}: (view)')\n",
    "\n",
    "result = {\n",
    "    'status': 'success',\n",
    "    'ctx_id': ctx_id,\n",
    "    'target_date': target_date,\n",
    "    'ctx_dir': str(ctx_dir),\n",
    "    'tables': [t[0] for t in tables]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "etl": {
   "name": "Forecast Review",
   "description": "Pulls VP, VOVI, CT metadata, and pipeline artifacts for forecast review",
   "variables": {
    "vp_file_path": {
     "type": "text",
     "label": "VP File Path",
     "default": ""
    },
    "aws_profile": {
     "type": "text",
     "label": "AWS Profile for S3",
     "default": "forecast-artifacts"
    }
   },
   "sources": [],
   "outputs": [],
   "options": {
    "return_preview": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
