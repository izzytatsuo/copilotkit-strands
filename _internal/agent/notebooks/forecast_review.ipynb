{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Forecast Review\n\nPost-publish review pipeline. Uses PUBLISHED VP data (local time CPTs).\nFetches CT metadata, filters stations by timezone bucket,\npulls published VP and VOVI data, downloads S3 artifacts, and produces joined.csv + visual.json.\n\nInputs (injected variables):\n- `tz_bucket` — Timezone bucket: Eastern, Central, Mountain, Pacific, etc. (default: Eastern)\n- `biz` — Business line: AMZL, AMXL, RSR (default: AMZL)\n- `aws_profile` — AWS profile for S3 (default: None)\n- `vovi_start_utc` — VOVI modified_time filter start, UTC epoch ms (default: 0 — all VOVI)\n- `vovi_end_utc` — VOVI modified_time filter end, UTC epoch ms (default: now — all VOVI)\n- `setup_ctx_dir` — Path to a previous setup context directory to import confidence data (default: None)\n\nData pulls:\n1. CT metadata from Control Tower API\n2. VP published (volume_plan) filtered by timezone bucket\n3. VOVI forecasts (US + CA)\n4. Pipeline artifacts from S3\n5. Intraday PBA data from S3\n\nTransforms:\n- VP pivot (long to wide, local→UTC conversion, util columns, grid keys)\n- CT site list + VP + VOVI outer join with available_inputs flag\n- Setup confidence LEFT JOIN (pre-publish confidence comparison)\n- PBA query (separate output file)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "setup",
    "type": "python"
   },
   "source": "from pathlib import Path\nfrom datetime import datetime, timedelta\nimport pytz\n\n# Accept injected variables or use defaults\nif 'tz_bucket' not in dir():\n    tz_bucket = 'Eastern'\nif 'biz' not in dir():\n    biz = 'AMZL'\nif 'aws_profile' not in dir():\n    aws_profile = None\n\n# VOVI modified_time filter (UTC epoch ms) — default: 0 to now (all VOVI)\nnow_utc_ms = int(datetime.now(pytz.utc).timestamp() * 1000)\nif 'vovi_start_utc' not in dir():\n    vovi_start_utc = 0\nif 'vovi_end_utc' not in dir():\n    vovi_end_utc = now_utc_ms\n\n# Setup context directory for importing pre-publish confidence data\nif 'setup_ctx_dir' not in dir():\n    setup_ctx_dir = None\n\n# Auto-calculate target date (tomorrow Pacific)\npacific = pytz.timezone('US/Pacific')\nnow_pacific = datetime.now(pacific)\ntomorrow = now_pacific + timedelta(days=1)\ntarget_date = tomorrow.strftime('%Y-%m-%d')\n\n# Generate context ID and output directory\nctx_id = generate_ctx_id(f'forecast_review_{tz_bucket.lower()}')\nctx_dir = contexts_dir / ctx_id\nctx_dir.mkdir(parents=True, exist_ok=True)\n\n# Create _vars table for SQL cells\nconn.execute(f\"CREATE OR REPLACE TABLE _vars AS SELECT '{tz_bucket}' as tz_bucket, '{biz}' as biz\")\n\nprint(f'Timezone: {tz_bucket}')\nprint(f'Business: {biz}')\nprint(f'Target date: {target_date}')\nprint(f'VOVI filter: {vovi_start_utc} - {vovi_end_utc} (epoch ms)')\nprint(f'Setup context: {setup_ctx_dir}')\nprint(f'Context ID: {ctx_id}')\nprint(f'Output: {ctx_dir}')\n\nresult = {'tz_bucket': tz_bucket, 'biz': biz, 'target_date': target_date, 'ctx_id': ctx_id, 'ctx_dir': str(ctx_dir), 'vovi_start_utc': vovi_start_utc, 'vovi_end_utc': vovi_end_utc, 'setup_ctx_dir': str(setup_ctx_dir) if setup_ctx_dir else None}",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "ct_fetch",
    "type": "python"
   },
   "source": "# name: ct_fetch | type: python\nimport json as _json\n\nr = _json.loads(fetch_ct_metadata(ctx_id))\nif r['success']:\n    conn.execute(f\"CREATE OR REPLACE TABLE ct_metadata AS SELECT * FROM '{r['output_file']}'\")\n    ct_count = conn.execute(\"SELECT COUNT(DISTINCT lm_node) FROM ct_metadata WHERE master_region = 'NA' AND status = 'A'\").fetchone()[0]\n    print(f'CT metadata: {r[\"row_count\"]} total rows, {ct_count} active NA stations')\nelse:\n    raise RuntimeError(f'CT metadata fetch failed: {r.get(\"error\", \"unknown\")}')\n\nresult = r",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "tz_bucket_map",
    "type": "sql"
   },
   "source": "-- name: tz_bucket_map | type: sql\nCREATE OR REPLACE TABLE tz_bucket_map AS\nWITH tomorrow_la AS (\n    SELECT CAST((current_timestamp AT TIME ZONE 'America/Los_Angeles') AS DATE) + 1 AS plan_date\n),\nutc_noon AS (\n    SELECT CAST((plan_date || ' 12:00:00')::VARCHAR AS TIMESTAMP WITHOUT TIME ZONE) AS utc_ts FROM tomorrow_la\n),\ntimezones AS (\n    SELECT DISTINCT timezone FROM ct_metadata WHERE master_region = 'NA'\n),\ntz_offsets AS (\n    SELECT timezone, datediff('HOUR', TIMEZONE('UTC', utc_ts AT TIME ZONE 'UTC'), TIMEZONE(timezone, utc_ts AT TIME ZONE 'UTC')) as utc_hour_diff\n    FROM utc_noon CROSS JOIN timezones\n),\nreference_buckets AS (\n    SELECT 'America/Los_Angeles' AS ref_timezone, 'pacific' AS bucket FROM utc_noon UNION ALL\n    SELECT 'America/Denver', 'mountain' FROM utc_noon UNION ALL\n    SELECT 'America/Chicago', 'central' FROM utc_noon UNION ALL\n    SELECT 'America/New_York', 'eastern' FROM utc_noon UNION ALL\n    SELECT 'America/Halifax', 'halifax' FROM utc_noon UNION ALL\n    SELECT 'America/Anchorage', 'alaska' FROM utc_noon UNION ALL\n    SELECT 'Pacific/Honolulu', 'hawaii' FROM utc_noon\n),\nreference_offsets AS (\n    SELECT rb.ref_timezone, rb.bucket,\n        datediff('HOUR', TIMEZONE('UTC', utc_ts AT TIME ZONE 'UTC'), TIMEZONE(rb.ref_timezone, utc_ts AT TIME ZONE 'UTC')) as ref_utc_offset\n    FROM utc_noon CROSS JOIN reference_buckets rb\n)\nSELECT tz.timezone, tz.utc_hour_diff, COALESCE(ro.bucket, 'other') AS bucket\nFROM tz_offsets tz LEFT JOIN reference_offsets ro ON tz.utc_hour_diff = ro.ref_utc_offset",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "load_site_list",
    "type": "sql"
   },
   "source": "-- name: load_site_list | type: sql\nCREATE OR REPLACE TABLE site_list AS\nSELECT DISTINCT\n    lm_node AS station,\n    'Single' AS cycle,\n    (SELECT biz FROM _vars) AS business_org\nFROM ct_metadata\nWHERE master_region = 'NA'\n  AND status = 'A'\n  AND timezone IN (SELECT timezone FROM tz_bucket_map WHERE bucket = LOWER((SELECT tz_bucket FROM _vars)))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "vp_urls",
    "type": "sql"
   },
   "source": "-- name: vp_urls | type: sql\nCREATE OR REPLACE TABLE vp_urls AS\nWITH plan_start AS (\n    SELECT CAST((current_timestamp AT TIME ZONE 'America/Los_Angeles') AS DATE) + 1 AS d\n)\nSELECT 'https://na.prod.control-tower.last-mile.amazon.dev/api/rap-dal/artifacts/NA/volume_planner/intraweek/volume_plan/1?scenario=POR&status=PUBLISHED&space=station%3D'\n    || station || '&time=plan_start_date%3D' || d || '&business=' || (SELECT biz FROM _vars) AS url\nFROM site_list CROSS JOIN plan_start",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "vp_fetch",
    "type": "python"
   },
   "source": "# name: vp_fetch | type: python\nimport json as _json\n\nurls = ','.join([r[0] for r in conn.execute('SELECT url FROM vp_urls').fetchall()])\nstation_count = conn.execute('SELECT COUNT(*) FROM vp_urls').fetchone()[0]\nprint(f'Fetching published VP data for {station_count} {tz_bucket} stations...')\n\nr = _json.loads(fetch_vp_pipeline(ctx_id, urls, 'published', 4))\nif r['success']:\n    conn.execute(f\"CREATE OR REPLACE TABLE vp_raw AS SELECT * FROM '{r['csv_file']}'\")\n    print(f'VP fetch: {r[\"fetched\"]} ok, {r[\"failed\"]} failed, {r[\"rows_transformed\"]} rows')\nelse:\n    raise RuntimeError(f'VP fetch failed: {r.get(\"error\", \"unknown\")}')\n\nresult = r",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "fetch_vovi",
    "type": "python"
   },
   "source": "# name: fetch_vovi | type: python\n# Fetch VOVI forecasts for US and CA\nimport subprocess\nimport json as _json\n\ncookie_path = str(Path.home() / '.midway' / 'cookie')\nvovi_base = 'https://prod.vovi.last-mile.amazon.dev/api/forecast/list_approved'\n\ncountries = ['US', 'CA']\nbusiness_type = biz.lower()\nshipping_type = 'premium'\n\nvovi_ctx_dir = ctx_dir / 'vovi'\nvovi_ctx_dir.mkdir(parents=True, exist_ok=True)\n\nvovi_results = []\n\nfor country in countries:\n    url = f'{vovi_base}?country={country}&cptDateKey={target_date}&shippingType={shipping_type}&businessType={business_type}'\n    print(f'Fetching VOVI: {country} / {business_type} / {shipping_type} / {target_date}...')\n    \n    try:\n        curl_result = subprocess.run(\n            ['curl.exe', '--location-trusted', '-b', cookie_path, '-s', url],\n            capture_output=True, text=True\n        )\n        \n        if curl_result.returncode != 0:\n            print(f'  {country}: curl failed - {curl_result.stderr[:100]}')\n            vovi_results.append({'country': country, 'success': False, 'error': curl_result.stderr[:200]})\n            continue\n        \n        data = _json.loads(curl_result.stdout)\n        df = pd.DataFrame(data)\n        \n        # Save to context directory\n        csv_file = vovi_ctx_dir / f'vovi_{country.lower()}_{business_type}_{shipping_type}.csv'\n        df.to_csv(csv_file, index=False)\n        \n        # Register in DuckDB\n        table_name = f'vovi_{country.lower()}'\n        conn.register(table_name, df)\n        \n        print(f'  {country}: {len(df):,} rows, {len(df.columns)} cols -> {table_name}')\n        vovi_results.append({'country': country, 'success': True, 'rows': len(df), 'table': table_name, 'path': str(csv_file)})\n        \n    except Exception as e:\n        print(f'  {country}: failed - {e}')\n        vovi_results.append({'country': country, 'success': False, 'error': str(e)})\n\n# Create combined vovi table with modified_time filter\ntry:\n    tables_to_union = [r['table'] for r in vovi_results if r.get('success')]\n    if tables_to_union:\n        union_sql = ' UNION ALL BY NAME '.join([f\"SELECT * FROM {t}\" for t in tables_to_union])\n        filter_clause = f\"WHERE modified_time >= {vovi_start_utc} AND modified_time <= {vovi_end_utc}\"\n        conn.execute(f'CREATE OR REPLACE VIEW vovi AS SELECT * FROM ({union_sql}) sub {filter_clause}')\n        vovi_fetched = sum(r['rows'] for r in vovi_results if r.get('success'))\n        vovi_total = conn.execute('SELECT COUNT(*) FROM vovi').fetchone()[0]\n        print(f'\\nCombined vovi: {vovi_fetched:,} fetched, {vovi_total:,} after modified_time filter')\nexcept Exception as e:\n    print(f'Could not create combined view: {e}')\n\nresult = vovi_results",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "download_artifacts",
    "type": "python"
   },
   "source": "# name: download_artifacts | type: python\n# Download latest pipeline artifacts from S3\nimport re\n\nartifact_bucket = 'lma-glue-pipeline'\nsort_code = 'DS-A'\nartifacts_dir = ctx_dir / 'pipeline_artifacts'\nartifacts_dir.mkdir(parents=True, exist_ok=True)\n\nsession = make_boto3_session(profile_name=aws_profile)\ns3 = session.client('s3')\n\ns3_prefix = f'pipeline_output/internal_sort_code={sort_code}/target_forecast_date={target_date}/'\nprint(f'Scanning: s3://{artifact_bucket}/{s3_prefix}')\n\n# Group by artifact type, keep latest timestamp\npaginator = s3.get_paginator('list_objects_v2')\nfiles_by_type = {}\n\nfor page in paginator.paginate(Bucket=artifact_bucket, Prefix=s3_prefix):\n    for obj in page.get('Contents', []):\n        key = obj['Key']\n        filename = key.split('/')[-1]\n        match = re.match(r'^(.+?)_(\\d{8}_\\d{6})(.*)\\.csv$', filename)\n        if match:\n            base_name = match.group(1)\n            timestamp = match.group(2)\n            suffix = match.group(3)\n            artifact_type = base_name + suffix\n            if artifact_type not in files_by_type or timestamp > files_by_type[artifact_type]['timestamp']:\n                files_by_type[artifact_type] = {\n                    'key': key, 'filename': filename, 'timestamp': timestamp,\n                    'size': obj['Size'], 'artifact_type': artifact_type\n                }\n\nprint(f'Found {len(files_by_type)} artifact types')\n\n# Download and register each\nartifact_results = []\nfor atype, info in sorted(files_by_type.items()):\n    dest_file = artifacts_dir / info['filename']\n    size_mb = info['size'] / 1024 / 1024\n    \n    try:\n        print(f'  {info[\"filename\"]} ({size_mb:.1f} MB)...', flush=True)\n        s3.download_file(artifact_bucket, info['key'], str(dest_file))\n        \n        # Register in DuckDB\n        conn.execute(f\"CREATE OR REPLACE TABLE {atype} AS SELECT * FROM read_csv_auto('{dest_file}')\")\n        row_count = conn.execute(f'SELECT COUNT(*) FROM {atype}').fetchone()[0]\n        \n        artifact_results.append({'artifact': atype, 'rows': row_count, 'size_mb': round(size_mb, 1), 'path': str(dest_file)})\n    except Exception as e:\n        print(f'    FAILED: {e}')\n        artifact_results.append({'artifact': atype, 'error': str(e)})\n\nprint(f'\\nDownloaded and registered {len([a for a in artifact_results if \"rows\" in a])}/{len(files_by_type)} artifacts')\n\nresult = artifact_results",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "download_pba",
    "type": "python"
   },
   "source": "# name: download_pba | type: python\n# Download latest intraday PBA parquet from S3 (last-mile-staging bucket)\nimport re as _re\n\npba_bucket = 'last-mile-staging'\nsort_code = 'DS-A'\npba_dir = ctx_dir / 'intraday_pba'\npba_dir.mkdir(parents=True, exist_ok=True)\n\npba_session = make_boto3_session(profile_name='last-mile-staging')\npba_s3 = pba_session.client('s3')\n\npba_prefix = f'intraday-pba/partition_internal_sort_code={sort_code}/partition_target_date={target_date}/'\nprint(f'Scanning: s3://{pba_bucket}/{pba_prefix}')\n\n# List all objects under target date to find run_time partitions\npaginator = pba_s3.get_paginator('list_objects_v2')\nrun_times = {}\n\nfor page in paginator.paginate(Bucket=pba_bucket, Prefix=pba_prefix):\n    for obj in page.get('Contents', []):\n        key = obj['Key']\n        match = _re.search(r'partition_run_time=(\\d+)/', key)\n        if match:\n            run_time = int(match.group(1))\n            run_times[run_time] = {'key': key, 'size': obj['Size']}\n\nif not run_times:\n    print(f'No intraday PBA data found for {target_date}')\n    result = {'success': False, 'error': f'No data for {target_date}'}\nelse:\n    latest_run_time = max(run_times)\n    latest = run_times[latest_run_time]\n    filename = latest['key'].split('/')[-1]\n    size_mb = latest['size'] / 1024 / 1024\n    dest_file = pba_dir / filename\n\n    print(f'Latest run_time: {latest_run_time}')\n    print(f'Downloading: {filename} ({size_mb:.1f} MB)...', flush=True)\n\n    pba_s3.download_file(pba_bucket, latest['key'], str(dest_file))\n\n    # Register in DuckDB\n    conn.execute(f\"CREATE OR REPLACE TABLE intraday_pba AS SELECT * FROM read_parquet('{dest_file}')\")\n    pba_count = conn.execute('SELECT COUNT(*) FROM intraday_pba').fetchone()[0]\n    pba_cols = [col[0] for col in conn.execute('DESCRIBE intraday_pba').fetchall()]\n\n    print(f'Intraday PBA: {pba_count:,} rows, {len(pba_cols)} columns')\n    print(f'Saved to: {dest_file}')\n\n    result = {\n        'success': True,\n        'run_time': latest_run_time,\n        'rows': pba_count,\n        'columns': pba_cols,\n        'size_mb': round(size_mb, 1),\n        'path': str(dest_file)\n    }",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# name: load_setup_confidence | type: python\n# Load confidence data from the previous setup run's joined.csv\n# If unavailable, create an empty table so downstream LEFT JOINs produce NULLs\n\nsetup_joined = Path(setup_ctx_dir) / 'joined.csv' if setup_ctx_dir else None\n\nif setup_joined and setup_joined.exists():\n    conn.execute(f\"\"\"\n        CREATE OR REPLACE TABLE setup_confidence AS\n        SELECT station, cpts_utc,\n               CAST(automated_confidence AS VARCHAR) AS setup_automated_confidence,\n               CAST(confidence_anomaly AS VARCHAR) AS setup_confidence_anomaly\n        FROM read_csv_auto('{setup_joined}')\n    \"\"\")\n    sc_count = conn.execute(\"SELECT COUNT(*) FROM setup_confidence\").fetchone()[0]\n    print(f'Setup confidence loaded: {sc_count} rows from {setup_joined}')\nelse:\n    conn.execute(\"\"\"\n        CREATE OR REPLACE TABLE setup_confidence (\n            station VARCHAR,\n            cpts_utc VARCHAR,\n            setup_automated_confidence VARCHAR,\n            setup_confidence_anomaly VARCHAR\n        )\n    \"\"\")\n    reason = 'no setup_ctx_dir provided' if not setup_ctx_dir else f'joined.csv not found in {setup_ctx_dir}'\n    print(f'Setup confidence: empty table ({reason})')\n\nresult = {'setup_confidence_loaded': bool(setup_joined and setup_joined.exists()), 'setup_ctx_dir': str(setup_ctx_dir) if setup_ctx_dir else None}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "vp_pivot",
    "type": "sql"
   },
   "source": "-- name: vp_pivot | type: sql\nSET TimeZone = 'UTC';\n\nCREATE OR REPLACE TABLE vp AS\nWITH pivot_all AS (\n    SELECT\n        node,\n        plan_start_date,\n        ofd_dates,\n        demand_types,\n        CAST(cpts AS TIMESTAMP) AS cpts,\n        MAX(CASE WHEN metric_name = 'total_volume_available' THEN metric_value END) AS total_volume_available,\n        MAX(CASE WHEN metric_name = 'automated_uncapped_slam_forecast' THEN metric_value END) AS automated_uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'current_slam' THEN metric_value END) AS current_slam,\n        MAX(CASE WHEN metric_name = 'weekly_uncapped_slam_forecast' THEN metric_value END) AS weekly_uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'post_cutoff_adjustment' THEN metric_value END) AS post_cutoff_adjustment,\n        MAX(CASE WHEN metric_name = 'total_backlog' THEN metric_value END) AS total_backlog,\n        MAX(CASE WHEN metric_name = 'automated_confidence' THEN metric_value END) AS automated_confidence,\n        MAX(CASE WHEN metric_name = 'uncapped_slam_forecast' THEN metric_value END) AS uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'atrops_soft_cap' THEN metric_value END) AS atrops_soft_cap,\n        MAX(CASE WHEN metric_name = 'confidence_anomaly' THEN metric_value END) AS confidence_anomaly,\n        MAX(CASE WHEN metric_name = 'net_volume_adjustments' THEN metric_value END) AS net_volume_adjustments,\n        MAX(CASE WHEN metric_name = 'adjusted_uncapped_slam_forecast' THEN metric_value END) AS adjusted_uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'cap_target_buffer' THEN metric_value END) AS cap_target_buffer,\n        MAX(CASE WHEN metric_name = 'earlies_expected' THEN metric_value END) AS earlies_expected,\n        MAX(CASE WHEN metric_name = 'returns' THEN metric_value END) AS returns,\n        MAX(CASE WHEN metric_name = 'sideline_in' THEN metric_value END) AS sideline_in,\n        MAX(CASE WHEN metric_name = 'vovi_uncapped_slam_forecast' THEN metric_value END) AS vovi_uncapped_slam_forecast,\n        MAX(CASE WHEN metric_name = 'in_station_backlog' THEN metric_value END) AS in_station_backlog,\n        MAX(CASE WHEN metric_name = 'mnr_expected' THEN metric_value END) AS mnr_expected,\n        MAX(CASE WHEN metric_name = 'mnr_received' THEN metric_value END) AS mnr_received,\n        MAX(CASE WHEN metric_name = 'current_schedule' THEN metric_value END) AS current_schedule,\n        MAX(CASE WHEN metric_name = 'vovi_adjustment' THEN metric_value END) AS vovi_adjustment,\n        MAX(CASE WHEN metric_name = 'forecast_type' THEN metric_value END) AS forecast_type,\n        MAX(CASE WHEN metric_name = 'earlies_received' THEN metric_value END) AS earlies_received,\n        MAX(CASE WHEN metric_name = 'latest_deployed_cap' THEN metric_value END) AS latest_deployed_cap,\n        MAX(CASE WHEN metric_name = 'atrops_hard_cap' THEN metric_value END) AS atrops_hard_cap,\n        MAX(CASE WHEN metric_name = 'capped_slam_forecast' THEN metric_value END) AS capped_slam_forecast\n    FROM vp_raw\n    WHERE plan_start_date::DATE = ofd_dates::DATE\n    GROUP BY node, plan_start_date, ofd_dates, demand_types, cpts\n)\nSELECT\n    p.*,\n    strftime(p.cpts, '%H:%M') AS cpts_local,\n    strftime(timezone(ct.timezone, p.cpts), '%H:%M') AS cpts_utc,\n    p.ofd_dates || '#' || p.node || '#' || strftime(p.cpts, '%H:%M') AS grid_key_local,\n    p.ofd_dates || '#' || p.node || '#' || strftime(timezone(ct.timezone, p.cpts), '%H:%M') AS grid_key_utc,\n    CASE WHEN GREATEST(COALESCE(p.latest_deployed_cap::FLOAT, 0), COALESCE(p.atrops_soft_cap::FLOAT, 0)) > 0\n         THEN ROUND(COALESCE(p.automated_uncapped_slam_forecast::FLOAT, 0) / GREATEST(COALESCE(p.latest_deployed_cap::FLOAT, 0), COALESCE(p.atrops_soft_cap::FLOAT, 0)), 4)\n         ELSE NULL END AS auto_forecast_util,\n    CASE WHEN GREATEST(COALESCE(p.latest_deployed_cap::FLOAT, 0), COALESCE(p.atrops_soft_cap::FLOAT, 0)) > 0\n         THEN ROUND(COALESCE(p.current_schedule::FLOAT, 0) / GREATEST(COALESCE(p.latest_deployed_cap::FLOAT, 0), COALESCE(p.atrops_soft_cap::FLOAT, 0)), 4)\n         ELSE NULL END AS util\nFROM pivot_all p\nLEFT JOIN ct_metadata ct ON p.node = ct.lm_node AND ct.master_region = 'NA' AND ct.status = 'A'\nORDER BY p.node, p.cpts",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "joined",
    "type": "sql"
   },
   "source": "-- name: joined | type: sql\nSET TimeZone = 'UTC';\n\nCREATE OR REPLACE TABLE joined AS\nWITH vovi_prepared AS (\n    SELECT\n        station,\n        CAST(timezone('UTC', to_timestamp(station_cpt)) AS TIMESTAMP) AS cpt_utc,\n        match_key\n    FROM vovi\n    WHERE match_key IS NOT NULL\n),\n-- Map station+cpt_utc -> cpt_time_local from cumulative\ncpt_map AS (\n    SELECT DISTINCT\n        station,\n        strftime(cpt_utc::TIMESTAMP, '%H:%M') AS cpts_utc,\n        cpt_time_local\n    FROM cumulative\n),\n-- Latest PBA quantiles per station+CPT (at max horizon)\nlatest_pba AS (\n    SELECT station, cpts_utc, pba_p10, pba_p30, pba_p50, pba_p70, pba_p90\n    FROM (\n        SELECT\n            station,\n            strftime(cpt_utc::TIMESTAMP, '%H:%M') AS cpts_utc,\n            cumulative_p10::FLOAT AS pba_p10,\n            cumulative_p30::FLOAT AS pba_p30,\n            cumulative_p50::FLOAT AS pba_p50,\n            cumulative_p70::FLOAT AS pba_p70,\n            cumulative_p90::FLOAT AS pba_p90,\n            ROW_NUMBER() OVER (PARTITION BY station, strftime(cpt_utc::TIMESTAMP, '%H:%M') ORDER BY horizon_rank_local DESC) AS rn\n        FROM cumulative\n    ) sub\n    WHERE rn = 1\n),\n-- Setup confidence from previous setup run (may be empty)\nsetup_conf AS (\n    SELECT DISTINCT station, cpts_utc, setup_automated_confidence, setup_confidence_anomaly\n    FROM setup_confidence\n),\nbase AS (\n    SELECT\n        COALESCE(sl.station, vp.node) AS station,\n        sl.cycle,\n        sl.business_org,\n        CASE\n            WHEN sl.station IS NOT NULL AND vp.node IS NOT NULL THEN 'vp_list'\n            WHEN vp.node IS NOT NULL THEN 'vp'\n            ELSE 'list'\n        END AS available_inputs,\n        vp.plan_start_date,\n        vp.ofd_dates,\n        vp.demand_types,\n        vp.cpts,\n        vp.cpts_local,\n        vp.cpts_utc,\n        vp.grid_key_local,\n        vp.grid_key_utc,\n        vp.forecast_type,\n        vp.automated_confidence,\n        vp.auto_forecast_util,\n        vp.util,\n        vp.vovi_uncapped_slam_forecast,\n        vp.uncapped_slam_forecast,\n        vp.adjusted_uncapped_slam_forecast,\n        vp.capped_slam_forecast,\n        vp.atrops_soft_cap,\n        vp.atrops_hard_cap,\n        vp.latest_deployed_cap,\n        vp.cap_target_buffer,\n        vp.current_slam,\n        vp.current_schedule,\n        vp.total_volume_available,\n        vp.total_backlog,\n        vp.in_station_backlog,\n        vp.post_cutoff_adjustment,\n        vp.net_volume_adjustments,\n        vp.vovi_adjustment,\n        vp.confidence_anomaly,\n        vp.automated_uncapped_slam_forecast,\n        vp.weekly_uncapped_slam_forecast,\n        vp.earlies_expected,\n        vp.earlies_received,\n        vp.returns,\n        vp.sideline_in,\n        vp.mnr_expected,\n        vp.mnr_received,\n        -- Setup confidence columns\n        sc.setup_automated_confidence,\n        sc.setup_confidence_anomaly,\n        v.modified_user AS vovi_modified_user,\n        -- VOVI modified_time converted to Central Time\n        CASE WHEN v.modified_time IS NOT NULL\n             THEN strftime(timezone('America/Chicago', to_timestamp(v.modified_time / 1000)), '%Y-%m-%d %H:%M:%S')\n             ELSE NULL END AS vovi_timestamp_ct,\n        v.proposed_cap AS vovi_proposed_cap,\n        v.post_cutoff_adjustment AS vovi_post_cutoff_adjustment,\n        v.adjusted_forecast AS vovi_adjusted_forecast,\n        v.forecast_source AS vovi_forecast_source,\n        v.original_forecast AS vovi_original_forecast,\n        v.forecast_status AS vovi_forecast_status,\n        v.forecast_adjustment AS vovi_forecast_adjustment,\n        v.current_slammed AS vovi_current_slammed,\n        v.current_scheduled AS vovi_current_scheduled,\n        v.soft_cap AS vovi_soft_cap,\n        v.hard_cap AS vovi_hard_cap,\n        v.match_key AS vovi_match_date,\n        -- Day classifier columns (from VOVI match date)\n        dc.bucket_lower,\n        dc.bucket_upper,\n        dc.peak_to_eod_drop_pct,\n        dc.constrained_after_target,\n        dc.sched_at_max_drop,\n        dc.max_drop_4hr,\n        dc.had_desched_notify,\n        dc.had_desched_execute,\n        -- Flatline flags (from target date)\n        fl.flatline_execute,\n        fl.flatline_notify,\n        -- Automated vs PBA quantile ratios (at latest horizon)\n        CASE WHEN lp.pba_p10 > 0 AND vp.automated_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.automated_uncapped_slam_forecast::FLOAT / lp.pba_p10, 2) END AS auto_vs_p10,\n        CASE WHEN lp.pba_p30 > 0 AND vp.automated_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.automated_uncapped_slam_forecast::FLOAT / lp.pba_p30, 2) END AS auto_vs_p30,\n        CASE WHEN lp.pba_p50 > 0 AND vp.automated_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.automated_uncapped_slam_forecast::FLOAT / lp.pba_p50, 2) END AS auto_vs_p50,\n        CASE WHEN lp.pba_p70 > 0 AND vp.automated_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.automated_uncapped_slam_forecast::FLOAT / lp.pba_p70, 2) END AS auto_vs_p70,\n        CASE WHEN lp.pba_p90 > 0 AND vp.automated_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.automated_uncapped_slam_forecast::FLOAT / lp.pba_p90, 2) END AS auto_vs_p90,\n        -- VOVI vs PBA quantile ratios (at latest horizon)\n        CASE WHEN lp.pba_p10 > 0 AND vp.vovi_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.vovi_uncapped_slam_forecast::FLOAT / lp.pba_p10, 2) END AS vovi_vs_p10,\n        CASE WHEN lp.pba_p30 > 0 AND vp.vovi_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.vovi_uncapped_slam_forecast::FLOAT / lp.pba_p30, 2) END AS vovi_vs_p30,\n        CASE WHEN lp.pba_p50 > 0 AND vp.vovi_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.vovi_uncapped_slam_forecast::FLOAT / lp.pba_p50, 2) END AS vovi_vs_p50,\n        CASE WHEN lp.pba_p70 > 0 AND vp.vovi_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.vovi_uncapped_slam_forecast::FLOAT / lp.pba_p70, 2) END AS vovi_vs_p70,\n        CASE WHEN lp.pba_p90 > 0 AND vp.vovi_uncapped_slam_forecast IS NOT NULL\n             THEN ROUND(vp.vovi_uncapped_slam_forecast::FLOAT / lp.pba_p90, 2) END AS vovi_vs_p90,\n        NOW()::TIMESTAMP AS execution_ts\n    FROM site_list sl\n    FULL OUTER JOIN vp ON sl.station = vp.node\n    LEFT JOIN vovi v\n        ON COALESCE(sl.station, vp.node) = v.station\n        AND vp.cpts_utc = strftime(CAST(timezone('UTC', to_timestamp(v.station_cpt)) AS TIMESTAMP), '%H:%M')\n    LEFT JOIN vovi_prepared vp2\n        ON COALESCE(sl.station, vp.node) = vp2.station\n        AND vp.cpts_utc = strftime(vp2.cpt_utc, '%H:%M')\n    LEFT JOIN cpt_map m\n        ON COALESCE(sl.station, vp.node) = m.station\n        AND vp.cpts_utc = m.cpts_utc\n    LEFT JOIN day_classifier dc\n        ON COALESCE(sl.station, vp.node) = dc.station\n        AND m.cpt_time_local = dc.cpt_time_local\n        AND vp2.match_key IS NOT NULL\n        AND dc.ofd_date = vp2.match_key::DATE\n    LEFT JOIN flatline_execute_flags fl\n        ON COALESCE(sl.station, vp.node) = fl.station\n        AND m.cpt_time_local = fl.cpt_time_local\n    LEFT JOIN latest_pba lp\n        ON COALESCE(sl.station, vp.node) = lp.station\n        AND vp.cpts_utc = lp.cpts_utc\n    LEFT JOIN setup_conf sc\n        ON COALESCE(sl.station, vp.node) = sc.station\n        AND vp.cpts_utc = sc.cpts_utc\n)\nSELECT\n    *,\n    -- Derived flags (comma-separated)\n    ARRAY_TO_STRING(LIST_VALUE(\n        CASE WHEN CAST(bucket_upper AS FLOAT) <= 10 THEN 'risk-under' END,\n        CASE WHEN CAST(flatline_execute AS INT) = 1 THEN 'flatline' END\n    ).list_filter(x -> x IS NOT NULL), ',') AS flags,\n    -- Confidence changed between setup and review (cast to VARCHAR — confidence_anomaly may be BOOLEAN)\n    CASE WHEN setup_confidence_anomaly IS NOT NULL\n         AND LOWER(COALESCE(CAST(confidence_anomaly AS VARCHAR), '')) != LOWER(COALESCE(CAST(setup_confidence_anomaly AS VARCHAR), ''))\n         THEN 'true' ELSE NULL END AS confidence_changed\nFROM base\nORDER BY station, cpts",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "summary",
    "type": "python"
   },
   "source": "# name: summary | type: python\n# Summary of available_inputs breakdown\nsummary = conn.execute(\"\"\"\n    SELECT available_inputs, COUNT(DISTINCT station) AS stations, COUNT(*) AS rows\n    FROM joined\n    GROUP BY available_inputs\n    ORDER BY available_inputs\n\"\"\").fetchdf()\nprint(summary.to_string(index=False))\n\ntotal = conn.execute(\"SELECT COUNT(*) FROM joined\").fetchone()[0]\ndistinct = conn.execute(\"SELECT COUNT(DISTINCT station) FROM joined\").fetchone()[0]\nprint(f'\\nTotal: {distinct} stations, {total} rows')\n\nresult = {\n    'status': 'success',\n    'total_rows': total,\n    'distinct_stations': distinct,\n    'breakdown': summary.to_dict('records')\n}",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "save_joined",
    "type": "python"
   },
   "source": "# name: save_joined | type: python\n# Export joined table to CSV\noutput_file = ctx_dir / 'joined.csv'\nconn.execute(f\"COPY joined TO '{output_file}' (HEADER, DELIMITER ',')\")\nrow_count = conn.execute(\"SELECT COUNT(*) FROM joined\").fetchone()[0]\nprint(f'Saved joined table: {row_count} rows -> {output_file}')\n\nresult = {'output_file': str(output_file), 'rows': row_count}",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "pba_visual",
    "type": "sql"
   },
   "source": "-- name: pba_visual | type: sql\nSET TimeZone = 'UTC';\n\nCREATE OR REPLACE TABLE pba_visual AS\nWITH vovi_prepared AS (\n    SELECT\n        *,\n        CAST(timezone('UTC', to_timestamp(station_cpt)) AS TIMESTAMP) AS cpt_utc\n    FROM vovi\n),\nvp_keys AS (\n    SELECT DISTINCT station, cpts_utc\n    FROM joined\n    WHERE cpts_utc IS NOT NULL\n),\nvp_target_date AS (\n    SELECT MAX(target_forecast_date::DATE) AS target_date FROM cumulative\n),\n\n-- Target date: cumulative data (has fan chart percentiles)\ntarget_rows AS (\n    SELECT\n        'target' AS pba_type,\n        (SELECT target_date FROM vp_target_date) || '#' || C.station || '#' || strftime(timezone(ct.timezone, timezone('UTC', C.cpt_utc::TIMESTAMP)), '%H:%M') AS grid_key,\n        C.target_forecast_date::VARCHAR AS pba_ofd_date,\n        C.dhm_horizon AS pba_dhm_horizon,\n        C.bi_hourly_trunc_local::VARCHAR AS pba_bi_hourly_local,\n        C.horizon_rank_local::FLOAT AS pba_horizon_rank,\n        C.scheduled::FLOAT AS pba_scheduled,\n        COALESCE(C.soft_cap::FLOAT, C.target_horizon_soft_cap::FLOAT, 0) AS pba_soft_cap,\n        I.hard_cap::FLOAT AS pba_hard_cap,\n        COALESCE(C.slammed::FLOAT, 0) AS pba_slammed,\n        I.cap_utilization::FLOAT AS pba_cap_utilization,\n        COALESCE(C.cumulative_median::FLOAT, 0) AS pba_cumulative_median,\n        COALESCE(C.cumulative_median_adj::FLOAT, 0) AS pba_cumulative_median_adj,\n        COALESCE(C.cumulative_p10::FLOAT, 0) AS pba_p10,\n        COALESCE(C.cumulative_p30::FLOAT, 0) AS pba_p30,\n        COALESCE(C.cumulative_p50::FLOAT, 0) AS pba_p50,\n        COALESCE(C.cumulative_p70::FLOAT, 0) AS pba_p70,\n        COALESCE(C.cumulative_p90::FLOAT, 0) AS pba_p90\n    FROM cumulative C\n    INNER JOIN vp_keys K\n        ON C.station = K.station\n        AND strftime(C.cpt_utc::TIMESTAMP, '%H:%M') = K.cpts_utc\n    LEFT JOIN ct_metadata ct\n        ON C.station = ct.lm_node AND ct.master_region = 'NA' AND ct.status = 'A'\n    LEFT JOIN intraday_pba I\n        ON C.station = I.station\n        AND C.cpt_utc::TIMESTAMP = I.cpt_utc\n        AND C.target_forecast_date::DATE = I.ofd_date\n        AND C.horizon_rank_local::FLOAT = I.horizon_rank::FLOAT\n    WHERE C.horizon_day > -2 OR (C.horizon_day = -2 AND C.horizon_hour >= 12)\n),\n\n-- Match date: intraday_pba via VOVI match_key (no fan chart data)\nmatch_rows AS (\n    SELECT\n        'match' AS pba_type,\n        (SELECT target_date FROM vp_target_date) || '#' || I.station || '#' || strftime(timezone(ct.timezone, timezone('UTC', I.cpt_utc::TIMESTAMP)), '%H:%M') AS grid_key,\n        I.ofd_date::VARCHAR AS pba_ofd_date,\n        I.dhm_horizon AS pba_dhm_horizon,\n        I.bi_hourly_trunc_local::VARCHAR AS pba_bi_hourly_local,\n        I.horizon_rank::FLOAT AS pba_horizon_rank,\n        COALESCE(I.scheduled::FLOAT, 0) AS pba_scheduled,\n        COALESCE(I.soft_cap::FLOAT, 0) AS pba_soft_cap,\n        COALESCE(I.hard_cap::FLOAT, 0) AS pba_hard_cap,\n        COALESCE(I.slammed::FLOAT, 0) AS pba_slammed,\n        COALESCE(I.cap_utilization::FLOAT, 0) AS pba_cap_utilization,\n        NULL::FLOAT AS pba_cumulative_median,\n        NULL::FLOAT AS pba_cumulative_median_adj,\n        NULL::FLOAT AS pba_p10,\n        NULL::FLOAT AS pba_p30,\n        NULL::FLOAT AS pba_p50,\n        NULL::FLOAT AS pba_p70,\n        NULL::FLOAT AS pba_p90\n    FROM vp_keys K\n    INNER JOIN vovi_prepared V\n        ON K.station = V.station\n        AND K.cpts_utc = strftime(V.cpt_utc, '%H:%M')\n    INNER JOIN intraday_pba I\n        ON V.station = I.station\n        AND strftime(V.cpt_utc, '%H:%M') = strftime(I.cpt_utc, '%H:%M')\n        AND V.match_key IS NOT NULL\n        AND I.ofd_date = V.match_key::DATE\n    LEFT JOIN ct_metadata ct\n        ON I.station = ct.lm_node AND ct.master_region = 'NA' AND ct.status = 'A'\n    WHERE I.horizon_day > -2 OR (I.horizon_day = -2 AND I.horizon_hour >= 12)\n),\n\n-- VP forecast reference markers at max horizon\nmax_horizons AS (\n    SELECT grid_key,\n           max_by(pba_dhm_horizon, pba_horizon_rank) AS max_dhm,\n           MAX(pba_horizon_rank) AS max_rank\n    FROM target_rows\n    GROUP BY grid_key\n),\nvp_markers AS (\n    SELECT\n        mk.pba_type,\n        mk.grid_key,\n        NULL::VARCHAR AS pba_ofd_date,\n        h.max_dhm AS pba_dhm_horizon,\n        NULL::VARCHAR AS pba_bi_hourly_local,\n        h.max_rank AS pba_horizon_rank,\n        mk.marker_value AS pba_scheduled,\n        NULL::FLOAT AS pba_soft_cap,\n        NULL::FLOAT AS pba_hard_cap,\n        NULL::FLOAT AS pba_slammed,\n        NULL::FLOAT AS pba_cap_utilization,\n        NULL::FLOAT AS pba_cumulative_median,\n        NULL::FLOAT AS pba_cumulative_median_adj,\n        NULL::FLOAT AS pba_p10,\n        NULL::FLOAT AS pba_p30,\n        NULL::FLOAT AS pba_p50,\n        NULL::FLOAT AS pba_p70,\n        NULL::FLOAT AS pba_p90\n    FROM (\n        SELECT 'vp_automated' AS pba_type, grid_key_local AS grid_key, automated_uncapped_slam_forecast::FLOAT AS marker_value\n        FROM joined WHERE grid_key_local IS NOT NULL AND automated_uncapped_slam_forecast IS NOT NULL\n        UNION ALL\n        SELECT 'vp_weekly', grid_key_local, weekly_uncapped_slam_forecast::FLOAT\n        FROM joined WHERE grid_key_local IS NOT NULL AND weekly_uncapped_slam_forecast IS NOT NULL\n        UNION ALL\n        SELECT 'vp_vovi', grid_key_local, vovi_uncapped_slam_forecast::FLOAT\n        FROM joined WHERE grid_key_local IS NOT NULL AND vovi_uncapped_slam_forecast IS NOT NULL\n    ) mk\n    INNER JOIN max_horizons h ON mk.grid_key = h.grid_key\n)\n\nSELECT * FROM target_rows\nUNION ALL\nSELECT * FROM match_rows\nUNION ALL\nSELECT * FROM vp_markers\nORDER BY grid_key, pba_type, pba_horizon_rank",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "save_pba_visual",
    "type": "python"
   },
   "source": "# name: save_pba_visual | type: python\n# Export PBA visual data to JSON for frontend chart consumption\nimport json as _json\nimport math as _math\n\npba_df = conn.execute(\"SELECT * FROM pba_visual\").fetchdf()\noutput_file = ctx_dir / 'visual.json'\n\n# Convert to list of dicts, replacing NaN with None for valid JSON\nrecords = pba_df.to_dict('records')\nfor rec in records:\n    for k, v in rec.items():\n        if isinstance(v, float) and _math.isnan(v):\n            rec[k] = None\n\nwith open(output_file, 'w') as f:\n    _json.dump(records, f, default=str)\n\nrow_count = len(records)\ndistinct_keys = pba_df['grid_key'].nunique()\ntarget_count = len(pba_df[pba_df['pba_type'] == 'target'])\nmatch_count = len(pba_df[pba_df['pba_type'] == 'match'])\nprint(f'Saved PBA visual data: {row_count} rows ({target_count} target, {match_count} match), {distinct_keys} grid_keys -> {output_file}')\n\nresult = {'output_file': str(output_file), 'rows': row_count, 'target': target_count, 'match': match_count, 'distinct_grid_keys': distinct_keys}",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "etl": {
   "name": "Forecast Review",
   "description": "Post-publish review pipeline. Uses published VP data with local time CPTs, converts to UTC for joins.",
   "variables": {
    "tz_bucket": {
     "type": "text",
     "label": "Timezone Bucket",
     "default": "Eastern"
    },
    "biz": {
     "type": "text",
     "label": "Business Line",
     "default": "AMZL"
    },
    "aws_profile": {
     "type": "text",
     "label": "AWS Profile for S3",
     "default": "forecast-artifacts"
    }
   },
   "sources": [],
   "outputs": [],
   "options": {
    "return_preview": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}