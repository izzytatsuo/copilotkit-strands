{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Pipeline Artifacts\n",
    "\n",
    "Downloads the latest run of each artifact type from the lma-glue-pipeline S3 bucket.\n",
    "\n",
    "**Artifacts downloaded:**\n",
    "- `flatline_execute` — Flatline execution results\n",
    "- `flatline_execute_flags` — Flatline flags\n",
    "- `flatline_keys` — Flatline keys\n",
    "- `flatline_signal` — Flatline signal data\n",
    "- `cumulative` — Cumulative forecast\n",
    "- `day_classifier` — Day classifier\n",
    "- `forecast` — Forecast output\n",
    "- `horizon_aggregates` — Horizon aggregates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "setup",
    "type": "python"
   },
   "source": [
    "# name: setup | type: python\n",
    "import boto3\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Variables injected from notebook metadata (can be overridden at runtime)\n",
    "# target_date, sort_code, bucket, aws_profile, output_dir are set by variable injection\n",
    "\n",
    "# Resolve output directory\n",
    "output_path = Path(output_dir).expanduser()\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Target date: {target_date}')\n",
    "print(f'Sort code: {sort_code}')\n",
    "print(f'Bucket: {bucket}')\n",
    "print(f'AWS profile: {aws_profile}')\n",
    "print(f'Output: {output_path}')\n",
    "\n",
    "result = {\n",
    "    'target_date': target_date,\n",
    "    'sort_code': sort_code,\n",
    "    'output_dir': str(output_path)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "list_artifacts",
    "type": "python"
   },
   "source": [
    "# name: list_artifacts | type: python\n",
    "session = boto3.Session(profile_name=aws_profile)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "prefix = f'pipeline_output/internal_sort_code={sort_code}/target_forecast_date={target_date}/'\n",
    "print(f'Scanning: s3://{bucket}/{prefix}')\n",
    "\n",
    "# List all objects and group by artifact type, keeping the latest per type\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "files_by_type = {}\n",
    "\n",
    "for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        filename = key.split('/')[-1]\n",
    "        \n",
    "        # Split on the timestamp pattern to get artifact type\n",
    "        # e.g., 'flatline_execute_20260208_190041_flags.csv' -> 'flatline_execute_flags'\n",
    "        # e.g., 'flatline_execute_20260208_190041.csv' -> 'flatline_execute'\n",
    "        import re\n",
    "        match = re.match(r'^(.+?)_(\\d{8}_\\d{6})(.*)\\.csv$', filename)\n",
    "        if match:\n",
    "            base_name = match.group(1)\n",
    "            timestamp = match.group(2)\n",
    "            suffix = match.group(3)  # e.g., '_flags' or ''\n",
    "            artifact_type = base_name + suffix\n",
    "            \n",
    "            if artifact_type not in files_by_type or timestamp > files_by_type[artifact_type]['timestamp']:\n",
    "                files_by_type[artifact_type] = {\n",
    "                    'key': key,\n",
    "                    'filename': filename,\n",
    "                    'timestamp': timestamp,\n",
    "                    'size': obj['Size'],\n",
    "                    'last_modified': str(obj['LastModified']),\n",
    "                    'artifact_type': artifact_type\n",
    "                }\n",
    "\n",
    "print(f'\\nFound {len(files_by_type)} artifact types:')\n",
    "for atype, info in sorted(files_by_type.items()):\n",
    "    size_mb = info['size'] / 1024 / 1024\n",
    "    print(f'  {atype}: {info[\"filename\"]} ({size_mb:.1f} MB)')\n",
    "\n",
    "total_mb = sum(f['size'] for f in files_by_type.values()) / 1024 / 1024\n",
    "print(f'\\nTotal: {total_mb:.1f} MB')\n",
    "\n",
    "result = {\n",
    "    'artifact_count': len(files_by_type),\n",
    "    'total_mb': round(total_mb, 1),\n",
    "    'artifacts': list(files_by_type.keys())\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "download",
    "type": "python"
   },
   "source": [
    "# name: download | type: python\n",
    "downloaded = []\n",
    "failed = []\n",
    "\n",
    "for atype, info in sorted(files_by_type.items()):\n",
    "    dest_file = output_path / info['filename']\n",
    "    size_mb = info['size'] / 1024 / 1024\n",
    "    \n",
    "    try:\n",
    "        print(f'Downloading {info[\"filename\"]} ({size_mb:.1f} MB)...', flush=True)\n",
    "        s3.download_file(bucket, info['key'], str(dest_file))\n",
    "        downloaded.append({\n",
    "            'artifact_type': atype,\n",
    "            'filename': info['filename'],\n",
    "            'size_mb': round(size_mb, 1),\n",
    "            'path': str(dest_file)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f'  FAILED: {e}')\n",
    "        failed.append({'artifact_type': atype, 'error': str(e)})\n",
    "\n",
    "print(f'\\nDownloaded: {len(downloaded)}/{len(files_by_type)}')\n",
    "if failed:\n",
    "    print(f'Failed: {len(failed)}')\n",
    "    for f in failed:\n",
    "        print(f'  {f[\"artifact_type\"]}: {f[\"error\"]}')\n",
    "\n",
    "result = {\n",
    "    'status': 'success' if not failed else 'partial',\n",
    "    'downloaded': len(downloaded),\n",
    "    'failed': len(failed),\n",
    "    'output_dir': str(output_path),\n",
    "    'files': downloaded\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "register_tables",
    "type": "python"
   },
   "source": [
    "# name: register_tables | type: python\n",
    "# Register downloaded CSVs as DuckDB tables for immediate querying\n",
    "registered = []\n",
    "\n",
    "for file_info in downloaded:\n",
    "    table_name = file_info['artifact_type']\n",
    "    file_path = file_info['path']\n",
    "    \n",
    "    try:\n",
    "        conn.execute(f\"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM read_csv_auto('{file_path}')\")\n",
    "        row_count = conn.execute(f'SELECT COUNT(*) FROM {table_name}').fetchone()[0]\n",
    "        registered.append({'table': table_name, 'rows': row_count})\n",
    "        print(f'  {table_name}: {row_count:,} rows')\n",
    "    except Exception as e:\n",
    "        print(f'  {table_name}: FAILED - {e}')\n",
    "\n",
    "print(f'\\nRegistered {len(registered)} tables in DuckDB')\n",
    "\n",
    "result = {\n",
    "    'status': 'success',\n",
    "    'tables_registered': len(registered),\n",
    "    'tables': registered,\n",
    "    'output_dir': str(output_path)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "etl": {
   "name": "Download Pipeline Artifacts",
   "description": "Downloads latest pipeline artifacts from S3 and registers as DuckDB tables",
   "variables": {
    "target_date": {
     "type": "text",
     "label": "Target Forecast Date",
     "default": "2026-02-09"
    },
    "sort_code": {
     "type": "text",
     "label": "Internal Sort Code",
     "default": "DS-A"
    },
    "bucket": {
     "type": "text",
     "label": "S3 Bucket",
     "default": "lma-glue-pipeline"
    },
    "aws_profile": {
     "type": "text",
     "label": "AWS Profile",
     "default": "forecast-artifacts"
    },
    "output_dir": {
     "type": "text",
     "label": "Output Directory",
     "default": "C:/Users/admsia/projects/pipeline-optimization/analysis/altkstorage"
    }
   },
   "sources": [],
   "outputs": [],
   "options": {
    "return_preview": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
