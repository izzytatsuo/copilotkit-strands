{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Forecast Setup (Timezone)\n",
    "\n",
    "Timezone-parameterized pipeline. Fetches CT metadata, filters stations by timezone bucket,\n",
    "pulls VP and VOVI data, downloads S3 artifacts, and produces joined.csv + visual.json.\n",
    "\n",
    "Inputs (injected variables):\n",
    "- `tz_bucket` \u00e2\u20ac\u201d Timezone bucket: Eastern, Central, Mountain, Pacific, etc. (default: Eastern)\n",
    "- `biz` \u00e2\u20ac\u201d Business line: AMZL, AMXL, RSR (default: AMZL)\n",
    "- `aws_profile` \u00e2\u20ac\u201d AWS profile for S3 (default: None)\n",
    "\n",
    "Data pulls:\n",
    "1. CT metadata from Control Tower API\n",
    "2. VP pipeline (unpublished) filtered by timezone bucket\n",
    "3. VOVI forecasts (US + CA)\n",
    "4. Pipeline artifacts from S3\n",
    "5. Intraday PBA data from S3\n",
    "\n",
    "Transforms:\n",
    "- VP pivot (long to wide, util columns, grid keys)\n",
    "- CT site list + VP + VOVI outer join with available_inputs flag\n",
    "- PBA query (separate output file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name: setup | type: python\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Accept injected variables or use defaults\n",
    "if 'tz_bucket' not in dir():\n",
    "    tz_bucket = 'Eastern'\n",
    "if 'biz' not in dir():\n",
    "    biz = 'AMZL'\n",
    "if 'aws_profile' not in dir():\n",
    "    aws_profile = None\n",
    "\n",
    "# Auto-calculate target date (tomorrow Pacific)\n",
    "pacific = pytz.timezone('US/Pacific')\n",
    "now_pacific = datetime.now(pacific)\n",
    "tomorrow = now_pacific + timedelta(days=1)\n",
    "target_date = tomorrow.strftime('%Y-%m-%d')\n",
    "\n",
    "# Generate context ID and output directory\n",
    "ctx_id = generate_ctx_id(f'forecast_{tz_bucket.lower()}')\n",
    "ctx_dir = contexts_dir / ctx_id\n",
    "ctx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create _vars table for SQL cells\n",
    "conn.execute(f\"CREATE OR REPLACE TABLE _vars AS SELECT '{tz_bucket}' as tz_bucket, '{biz}' as biz\")\n",
    "\n",
    "print(f'Timezone: {tz_bucket}')\n",
    "print(f'Business: {biz}')\n",
    "print(f'Target date: {target_date}')\n",
    "print(f'Context ID: {ctx_id}')\n",
    "print(f'Output: {ctx_dir}')\n",
    "\n",
    "result = {'tz_bucket': tz_bucket, 'biz': biz, 'target_date': target_date, 'ctx_id': ctx_id, 'ctx_dir': str(ctx_dir)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name: ct_fetch | type: python\n",
    "import json as _json\n",
    "\n",
    "r = _json.loads(fetch_ct_metadata(ctx_id))\n",
    "if r['success']:\n",
    "    conn.execute(f\"CREATE OR REPLACE TABLE ct_metadata AS SELECT * FROM '{r['output_file']}'\")\n",
    "    ct_count = conn.execute(\"SELECT COUNT(DISTINCT lm_node) FROM ct_metadata WHERE master_region = 'NA' AND status = 'A'\").fetchone()[0]\n",
    "    print(f'CT metadata: {r[\"row_count\"]} total rows, {ct_count} active NA stations')\n",
    "else:\n",
    "    raise RuntimeError(f'CT metadata fetch failed: {r.get(\"error\", \"unknown\")}')\n",
    "\n",
    "result = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- name: tz_bucket_map | type: sql\n",
    "CREATE OR REPLACE TABLE tz_bucket_map AS\n",
    "WITH tomorrow_la AS (\n",
    "    SELECT CAST((current_timestamp AT TIME ZONE 'America/Los_Angeles') AS DATE) + 1 AS plan_date\n",
    "),\n",
    "utc_noon AS (\n",
    "    SELECT CAST((plan_date || ' 12:00:00')::VARCHAR AS TIMESTAMP WITHOUT TIME ZONE) AS utc_ts FROM tomorrow_la\n",
    "),\n",
    "timezones AS (\n",
    "    SELECT DISTINCT timezone FROM ct_metadata WHERE master_region = 'NA'\n",
    "),\n",
    "tz_offsets AS (\n",
    "    SELECT timezone, datediff('HOUR', TIMEZONE('UTC', utc_ts AT TIME ZONE 'UTC'), TIMEZONE(timezone, utc_ts AT TIME ZONE 'UTC')) as utc_hour_diff\n",
    "    FROM utc_noon CROSS JOIN timezones\n",
    "),\n",
    "reference_buckets AS (\n",
    "    SELECT 'America/Los_Angeles' AS ref_timezone, 'pacific' AS bucket FROM utc_noon UNION ALL\n",
    "    SELECT 'America/Denver', 'mountain' FROM utc_noon UNION ALL\n",
    "    SELECT 'America/Chicago', 'central' FROM utc_noon UNION ALL\n",
    "    SELECT 'America/New_York', 'eastern' FROM utc_noon UNION ALL\n",
    "    SELECT 'America/Halifax', 'halifax' FROM utc_noon UNION ALL\n",
    "    SELECT 'America/Anchorage', 'alaska' FROM utc_noon UNION ALL\n",
    "    SELECT 'Pacific/Honolulu', 'hawaii' FROM utc_noon\n",
    "),\n",
    "reference_offsets AS (\n",
    "    SELECT rb.ref_timezone, rb.bucket,\n",
    "        datediff('HOUR', TIMEZONE('UTC', utc_ts AT TIME ZONE 'UTC'), TIMEZONE(rb.ref_timezone, utc_ts AT TIME ZONE 'UTC')) as ref_utc_offset\n",
    "    FROM utc_noon CROSS JOIN reference_buckets rb\n",
    ")\n",
    "SELECT tz.timezone, tz.utc_hour_diff, COALESCE(ro.bucket, 'other') AS bucket\n",
    "FROM tz_offsets tz LEFT JOIN reference_offsets ro ON tz.utc_hour_diff = ro.ref_utc_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- name: load_site_list | type: sql\n",
    "CREATE OR REPLACE TABLE site_list AS\n",
    "SELECT DISTINCT\n",
    "    lm_node AS station,\n",
    "    'Single' AS cycle,\n",
    "    (SELECT biz FROM _vars) AS business_org\n",
    "FROM ct_metadata\n",
    "WHERE master_region = 'NA'\n",
    "  AND status = 'A'\n",
    "  AND timezone IN (SELECT timezone FROM tz_bucket_map WHERE bucket = LOWER((SELECT tz_bucket FROM _vars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- name: vp_urls | type: sql\n",
    "CREATE OR REPLACE TABLE vp_urls AS\n",
    "WITH plan_start AS (\n",
    "    SELECT CAST((current_timestamp AT TIME ZONE 'America/Los_Angeles') AS DATE) + 1 AS d\n",
    ")\n",
    "SELECT 'https://na.prod.control-tower.last-mile.amazon.dev/api/rap-dal/artifacts/NA/volume_planner/intraweek/pipeline_volume_plan/1?scenario=POR&status=UNPUBLISHED&space=station%3D'\n",
    "    || station || '&time=plan_start_date%3D' || d || '&business=' || (SELECT biz FROM _vars) AS url\n",
    "FROM site_list CROSS JOIN plan_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name: vp_fetch | type: python\n",
    "import json as _json\n",
    "\n",
    "urls = ','.join([r[0] for r in conn.execute('SELECT url FROM vp_urls').fetchall()])\n",
    "station_count = conn.execute('SELECT COUNT(*) FROM vp_urls').fetchone()[0]\n",
    "print(f'Fetching VP data for {station_count} {tz_bucket} stations...')\n",
    "\n",
    "r = _json.loads(fetch_vp_pipeline(ctx_id, urls, 'unpublished', 4))\n",
    "if r['success']:\n",
    "    conn.execute(f\"CREATE OR REPLACE TABLE vp_raw AS SELECT * FROM '{r['csv_file']}'\")\n",
    "    print(f'VP fetch: {r[\"fetched\"]} ok, {r[\"failed\"]} failed, {r[\"rows_transformed\"]} rows')\n",
    "else:\n",
    "    raise RuntimeError(f'VP fetch failed: {r.get(\"error\", \"unknown\")}')\n",
    "\n",
    "result = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name: fetch_vovi | type: python\n",
    "# Fetch VOVI forecasts for US and CA\n",
    "import subprocess\n",
    "import json as _json\n",
    "\n",
    "cookie_path = str(Path.home() / '.midway' / 'cookie')\n",
    "vovi_base = 'https://prod.vovi.last-mile.amazon.dev/api/forecast/list_approved'\n",
    "\n",
    "countries = ['US', 'CA']\n",
    "business_type = biz.lower()\n",
    "shipping_type = 'premium'\n",
    "\n",
    "vovi_ctx_dir = ctx_dir / 'vovi'\n",
    "vovi_ctx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vovi_results = []\n",
    "\n",
    "for country in countries:\n",
    "    url = f'{vovi_base}?country={country}&cptDateKey={target_date}&shippingType={shipping_type}&businessType={business_type}'\n",
    "    print(f'Fetching VOVI: {country} / {business_type} / {shipping_type} / {target_date}...')\n",
    "    \n",
    "    try:\n",
    "        curl_result = subprocess.run(\n",
    "            ['curl.exe', '--location-trusted', '-b', cookie_path, '-s', url],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        \n",
    "        if curl_result.returncode != 0:\n",
    "            print(f'  {country}: curl failed - {curl_result.stderr[:100]}')\n",
    "            vovi_results.append({'country': country, 'success': False, 'error': curl_result.stderr[:200]})\n",
    "            continue\n",
    "        \n",
    "        data = _json.loads(curl_result.stdout)\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Save to context directory\n",
    "        csv_file = vovi_ctx_dir / f'vovi_{country.lower()}_{business_type}_{shipping_type}.csv'\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        # Register in DuckDB\n",
    "        table_name = f'vovi_{country.lower()}'\n",
    "        conn.register(table_name, df)\n",
    "        \n",
    "        print(f'  {country}: {len(df):,} rows, {len(df.columns)} cols -> {table_name}')\n",
    "        vovi_results.append({'country': country, 'success': True, 'rows': len(df), 'table': table_name, 'path': str(csv_file)})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'  {country}: failed - {e}')\n",
    "        vovi_results.append({'country': country, 'success': False, 'error': str(e)})\n",
    "\n",
    "# Create combined vovi table using UNION ALL BY NAME (handles differing column counts)\n",
    "try:\n",
    "    tables_to_union = [r['table'] for r in vovi_results if r.get('success')]\n",
    "    if tables_to_union:\n",
    "        union_sql = ' UNION ALL BY NAME '.join([f\"SELECT * FROM {t}\" for t in tables_to_union])\n",
    "        conn.execute(f'CREATE OR REPLACE VIEW vovi AS {union_sql}')\n",
    "        vovi_total = conn.execute('SELECT COUNT(*) FROM vovi').fetchone()[0]\n",
    "        print(f'\\nCombined vovi view: {vovi_total:,} rows')\n",
    "except Exception as e:\n",
    "    print(f'Could not create combined view: {e}')\n",
    "\n",
    "result = vovi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name: download_artifacts | type: python\n",
    "# Download latest pipeline artifacts from S3\n",
    "import re\n",
    "\n",
    "artifact_bucket = 'lma-glue-pipeline'\n",
    "sort_code = 'DS-A'\n",
    "artifacts_dir = ctx_dir / 'pipeline_artifacts'\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "session = make_boto3_session(profile_name=aws_profile)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "s3_prefix = f'pipeline_output/internal_sort_code={sort_code}/target_forecast_date={target_date}/'\n",
    "print(f'Scanning: s3://{artifact_bucket}/{s3_prefix}')\n",
    "\n",
    "# Group by artifact type, keep latest timestamp\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "files_by_type = {}\n",
    "\n",
    "for page in paginator.paginate(Bucket=artifact_bucket, Prefix=s3_prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        filename = key.split('/')[-1]\n",
    "        match = re.match(r'^(.+?)_(\\d{8}_\\d{6})(.*)\\.csv$', filename)\n",
    "        if match:\n",
    "            base_name = match.group(1)\n",
    "            timestamp = match.group(2)\n",
    "            suffix = match.group(3)\n",
    "            artifact_type = base_name + suffix\n",
    "            if artifact_type not in files_by_type or timestamp > files_by_type[artifact_type]['timestamp']:\n",
    "                files_by_type[artifact_type] = {\n",
    "                    'key': key, 'filename': filename, 'timestamp': timestamp,\n",
    "                    'size': obj['Size'], 'artifact_type': artifact_type\n",
    "                }\n",
    "\n",
    "print(f'Found {len(files_by_type)} artifact types')\n",
    "\n",
    "# Download and register each\n",
    "artifact_results = []\n",
    "for atype, info in sorted(files_by_type.items()):\n",
    "    dest_file = artifacts_dir / info['filename']\n",
    "    size_mb = info['size'] / 1024 / 1024\n",
    "    \n",
    "    try:\n",
    "        print(f'  {info[\"filename\"]} ({size_mb:.1f} MB)...', flush=True)\n",
    "        s3.download_file(artifact_bucket, info['key'], str(dest_file))\n",
    "        \n",
    "        # Register in DuckDB\n",
    "        conn.execute(f\"CREATE OR REPLACE TABLE {atype} AS SELECT * FROM read_csv_auto('{dest_file}')\")\n",
    "        row_count = conn.execute(f'SELECT COUNT(*) FROM {atype}').fetchone()[0]\n",
    "        \n",
    "        artifact_results.append({'artifact': atype, 'rows': row_count, 'size_mb': round(size_mb, 1), 'path': str(dest_file)})\n",
    "    except Exception as e:\n",
    "        print(f'    FAILED: {e}')\n",
    "        artifact_results.append({'artifact': atype, 'error': str(e)})\n",
    "\n",
    "print(f'\\nDownloaded and registered {len([a for a in artifact_results if \"rows\" in a])}/{len(files_by_type)} artifacts')\n",
    "\n",
    "result = artifact_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name: download_pba | type: python\n",
    "# Download latest intraday PBA parquet from S3 (last-mile-staging bucket)\n",
    "import re as _re\n",
    "\n",
    "pba_bucket = 'last-mile-staging'\n",
    "sort_code = 'DS-A'\n",
    "pba_dir = ctx_dir / 'intraday_pba'\n",
    "pba_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pba_session = make_boto3_session(profile_name='last-mile-staging')\n",
    "pba_s3 = pba_session.client('s3')\n",
    "\n",
    "pba_prefix = f'intraday-pba/partition_internal_sort_code={sort_code}/partition_target_date={target_date}/'\n",
    "print(f'Scanning: s3://{pba_bucket}/{pba_prefix}')\n",
    "\n",
    "# List all objects under target date to find run_time partitions\n",
    "paginator = pba_s3.get_paginator('list_objects_v2')\n",
    "run_times = {}\n",
    "\n",
    "for page in paginator.paginate(Bucket=pba_bucket, Prefix=pba_prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        match = _re.search(r'partition_run_time=(\\d+)/', key)\n",
    "        if match:\n",
    "            run_time = int(match.group(1))\n",
    "            run_times[run_time] = {'key': key, 'size': obj['Size']}\n",
    "\n",
    "if not run_times:\n",
    "    print(f'No intraday PBA data found for {target_date}')\n",
    "    result = {'success': False, 'error': f'No data for {target_date}'}\n",
    "else:\n",
    "    latest_run_time = max(run_times)\n",
    "    latest = run_times[latest_run_time]\n",
    "    filename = latest['key'].split('/')[-1]\n",
    "    size_mb = latest['size'] / 1024 / 1024\n",
    "    dest_file = pba_dir / filename\n",
    "\n",
    "    print(f'Latest run_time: {latest_run_time}')\n",
    "    print(f'Downloading: {filename} ({size_mb:.1f} MB)...', flush=True)\n",
    "\n",
    "    pba_s3.download_file(pba_bucket, latest['key'], str(dest_file))\n",
    "\n",
    "    # Register in DuckDB\n",
    "    conn.execute(f\"CREATE OR REPLACE TABLE intraday_pba AS SELECT * FROM read_parquet('{dest_file}')\")\n",
    "    pba_count = conn.execute('SELECT COUNT(*) FROM intraday_pba').fetchone()[0]\n",
    "    pba_cols = [col[0] for col in conn.execute('DESCRIBE intraday_pba').fetchall()]\n",
    "\n",
    "    print(f'Intraday PBA: {pba_count:,} rows, {len(pba_cols)} columns')\n",
    "    print(f'Saved to: {dest_file}')\n",
    "\n",
    "    result = {\n",
    "        'success': True,\n",
    "        'run_time': latest_run_time,\n",
    "        'rows': pba_count,\n",
    "        'columns': pba_cols,\n",
    "        'size_mb': round(size_mb, 1),\n",
    "        'path': str(dest_file)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- name: vp_pivot | type: sql\n",
    "SET TimeZone = 'UTC';\n",
    "\n",
    "CREATE OR REPLACE TABLE vp AS\n",
    "WITH pivot_all AS (\n",
    "    SELECT\n",
    "        node,\n",
    "        plan_start_date,\n",
    "        ofd_dates,\n",
    "        demand_types,\n",
    "        CAST(cpts AS TIMESTAMP) AS cpts,\n",
    "        MAX(CASE WHEN metric_name = 'total_volume_available' THEN metric_value END) AS total_volume_available,\n",
    "        MAX(CASE WHEN metric_name = 'automated_uncapped_slam_forecast' THEN metric_value END) AS automated_uncapped_slam_forecast,\n",
    "        MAX(CASE WHEN metric_name = 'current_slam' THEN metric_value END) AS current_slam,\n",
    "        MAX(CASE WHEN metric_name = 'weekly_uncapped_slam_forecast' THEN metric_value END) AS weekly_uncapped_slam_forecast,\n",
    "        MAX(CASE WHEN metric_name = 'post_cutoff_adjustment' THEN metric_value END) AS post_cutoff_adjustment,\n",
    "        MAX(CASE WHEN metric_name = 'total_backlog' THEN metric_value END) AS total_backlog,\n",
    "        MAX(CASE WHEN metric_name = 'automated_confidence' THEN metric_value END) AS automated_confidence,\n",
    "        MAX(CASE WHEN metric_name = 'uncapped_slam_forecast' THEN metric_value END) AS uncapped_slam_forecast,\n",
    "        MAX(CASE WHEN metric_name = 'atrops_soft_cap' THEN metric_value END) AS atrops_soft_cap,\n",
    "        MAX(CASE WHEN metric_name = 'confidence_anomaly' THEN metric_value END) AS confidence_anomaly,\n",
    "        MAX(CASE WHEN metric_name = 'net_volume_adjustments' THEN metric_value END) AS net_volume_adjustments,\n",
    "        MAX(CASE WHEN metric_name = 'adjusted_uncapped_slam_forecast' THEN metric_value END) AS adjusted_uncapped_slam_forecast,\n",
    "        MAX(CASE WHEN metric_name = 'cap_target_buffer' THEN metric_value END) AS cap_target_buffer,\n",
    "        MAX(CASE WHEN metric_name = 'earlies_expected' THEN metric_value END) AS earlies_expected,\n",
    "        MAX(CASE WHEN metric_name = 'returns' THEN metric_value END) AS returns,\n",
    "        MAX(CASE WHEN metric_name = 'sideline_in' THEN metric_value END) AS sideline_in,\n",
    "        MAX(CASE WHEN metric_name = 'vovi_uncapped_slam_forecast' THEN metric_value END) AS vovi_uncapped_slam_forecast,\n",
    "        MAX(CASE WHEN metric_name = 'in_station_backlog' THEN metric_value END) AS in_station_backlog,\n",
    "        MAX(CASE WHEN metric_name = 'mnr_expected' THEN metric_value END) AS mnr_expected,\n",
    "        MAX(CASE WHEN metric_name = 'mnr_received' THEN metric_value END) AS mnr_received,\n",
    "        MAX(CASE WHEN metric_name = 'current_schedule' THEN metric_value END) AS current_schedule,\n",
    "        MAX(CASE WHEN metric_name = 'vovi_adjustment' THEN metric_value END) AS vovi_adjustment,\n",
    "        MAX(CASE WHEN metric_name = 'forecast_type' THEN metric_value END) AS forecast_type,\n",
    "        MAX(CASE WHEN metric_name = 'earlies_received' THEN metric_value END) AS earlies_received,\n",
    "        MAX(CASE WHEN metric_name = 'latest_deployed_cap' THEN metric_value END) AS latest_deployed_cap,\n",
    "        MAX(CASE WHEN metric_name = 'atrops_hard_cap' THEN metric_value END) AS atrops_hard_cap,\n",
    "        MAX(CASE WHEN metric_name = 'capped_slam_forecast' THEN metric_value END) AS capped_slam_forecast\n",
    "    FROM vp_raw\n",
    "    WHERE plan_start_date::DATE = ofd_dates::DATE\n",
    "    GROUP BY node, plan_start_date, ofd_dates, demand_types, cpts\n",
    ")\n",
    "SELECT\n",
    "    *,\n",
    "    strftime(cpts, '%H:%M') AS cpts_local,\n",
    "    strftime(cpts AT TIME ZONE 'UTC', '%H:%M') AS cpts_utc,\n",
    "    ofd_dates || '#' || node || '#' || strftime(cpts, '%H:%M') AS grid_key_local,\n",
    "    ofd_dates || '#' || node || '#' || strftime(cpts AT TIME ZONE 'UTC', '%H:%M') AS grid_key_utc,\n",
    "    CASE WHEN GREATEST(COALESCE(latest_deployed_cap::FLOAT, 0), COALESCE(atrops_soft_cap::FLOAT, 0)) > 0\n",
    "         THEN ROUND(COALESCE(automated_uncapped_slam_forecast::FLOAT, 0) / GREATEST(COALESCE(latest_deployed_cap::FLOAT, 0), COALESCE(atrops_soft_cap::FLOAT, 0)), 4)\n",
    "         ELSE NULL END AS auto_forecast_util,\n",
    "    CASE WHEN GREATEST(COALESCE(latest_deployed_cap::FLOAT, 0), COALESCE(atrops_soft_cap::FLOAT, 0)) > 0\n",
    "         THEN ROUND(COALESCE(current_schedule::FLOAT, 0) / GREATEST(COALESCE(latest_deployed_cap::FLOAT, 0), COALESCE(atrops_soft_cap::FLOAT, 0)), 4)\n",
    "         ELSE NULL END AS util\n",
    "FROM pivot_all\n",
    "ORDER BY node, cpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- name: joined | type: sql\n",
    "SET TimeZone = 'UTC';\n",
    "\n",
    "CREATE OR REPLACE TABLE joined AS\n",
    "WITH vovi_prepared AS (\n",
    "    SELECT\n",
    "        station,\n",
    "        CAST(timezone('UTC', to_timestamp(station_cpt)) AS TIMESTAMP) AS cpt_utc,\n",
    "        match_key\n",
    "    FROM vovi\n",
    "    WHERE match_key IS NOT NULL\n",
    "),\n",
    "-- Map station+cpt_utc -> cpt_time_local from cumulative\n",
    "cpt_map AS (\n",
    "    SELECT DISTINCT\n",
    "        station,\n",
    "        strftime(cpt_utc::TIMESTAMP, '%H:%M') AS cpts_utc,\n",
    "        cpt_time_local\n",
    "    FROM cumulative\n",
    "),\n",
    "base AS (\n",
    "    SELECT\n",
    "        COALESCE(sl.station, vp.node) AS station,\n",
    "        sl.cycle,\n",
    "        sl.business_org,\n",
    "        CASE\n",
    "            WHEN sl.station IS NOT NULL AND vp.node IS NOT NULL THEN 'vp_list'\n",
    "            WHEN vp.node IS NOT NULL THEN 'vp'\n",
    "            ELSE 'list'\n",
    "        END AS available_inputs,\n",
    "        vp.plan_start_date,\n",
    "        vp.ofd_dates,\n",
    "        vp.demand_types,\n",
    "        vp.cpts,\n",
    "        vp.cpts_local,\n",
    "        vp.cpts_utc,\n",
    "        vp.grid_key_local,\n",
    "        vp.grid_key_utc,\n",
    "        vp.forecast_type,\n",
    "        vp.automated_confidence,\n",
    "        vp.auto_forecast_util,\n",
    "        vp.util,\n",
    "        vp.vovi_uncapped_slam_forecast,\n",
    "        vp.uncapped_slam_forecast,\n",
    "        vp.adjusted_uncapped_slam_forecast,\n",
    "        vp.capped_slam_forecast,\n",
    "        vp.atrops_soft_cap,\n",
    "        vp.atrops_hard_cap,\n",
    "        vp.latest_deployed_cap,\n",
    "        vp.cap_target_buffer,\n",
    "        vp.current_slam,\n",
    "        vp.current_schedule,\n",
    "        vp.total_volume_available,\n",
    "        vp.total_backlog,\n",
    "        vp.in_station_backlog,\n",
    "        vp.post_cutoff_adjustment,\n",
    "        vp.net_volume_adjustments,\n",
    "        vp.vovi_adjustment,\n",
    "        vp.confidence_anomaly,\n",
    "        vp.automated_uncapped_slam_forecast,\n",
    "        vp.weekly_uncapped_slam_forecast,\n",
    "        vp.earlies_expected,\n",
    "        vp.earlies_received,\n",
    "        vp.returns,\n",
    "        vp.sideline_in,\n",
    "        vp.mnr_expected,\n",
    "        vp.mnr_received,\n",
    "        v.modified_user AS vovi_modified_user,\n",
    "        v.proposed_cap AS vovi_proposed_cap,\n",
    "        v.post_cutoff_adjustment AS vovi_post_cutoff_adjustment,\n",
    "        v.adjusted_forecast AS vovi_adjusted_forecast,\n",
    "        v.forecast_source AS vovi_forecast_source,\n",
    "        v.original_forecast AS vovi_original_forecast,\n",
    "        v.forecast_status AS vovi_forecast_status,\n",
    "        v.forecast_adjustment AS vovi_forecast_adjustment,\n",
    "        v.current_slammed AS vovi_current_slammed,\n",
    "        v.current_scheduled AS vovi_current_scheduled,\n",
    "        v.soft_cap AS vovi_soft_cap,\n",
    "        v.hard_cap AS vovi_hard_cap,\n",
    "        -- Day classifier columns (from VOVI match date)\n",
    "        dc.bucket_lower,\n",
    "        dc.bucket_upper,\n",
    "        dc.peak_to_eod_drop_pct,\n",
    "        dc.constrained_after_target,\n",
    "        dc.sched_at_max_drop,\n",
    "        dc.max_drop_4hr,\n",
    "        dc.had_desched_notify,\n",
    "        dc.had_desched_execute,\n",
    "        -- Flatline flags (from target date)\n",
    "        fl.flatline_execute,\n",
    "        fl.flatline_notify,\n",
    "        NOW()::TIMESTAMP AS execution_ts\n",
    "    FROM site_list sl\n",
    "    FULL OUTER JOIN vp ON sl.station = vp.node\n",
    "    LEFT JOIN vovi v\n",
    "        ON COALESCE(sl.station, vp.node) = v.station\n",
    "        AND vp.cpts_utc = strftime(CAST(timezone('UTC', to_timestamp(v.station_cpt)) AS TIMESTAMP), '%H:%M')\n",
    "    LEFT JOIN vovi_prepared vp2\n",
    "        ON COALESCE(sl.station, vp.node) = vp2.station\n",
    "        AND vp.cpts_utc = strftime(vp2.cpt_utc, '%H:%M')\n",
    "    LEFT JOIN cpt_map m\n",
    "        ON COALESCE(sl.station, vp.node) = m.station\n",
    "        AND vp.cpts_utc = m.cpts_utc\n",
    "    LEFT JOIN day_classifier dc\n",
    "        ON COALESCE(sl.station, vp.node) = dc.station\n",
    "        AND m.cpt_time_local = dc.cpt_time_local\n",
    "        AND vp2.match_key IS NOT NULL\n",
    "        AND dc.ofd_date = vp2.match_key::DATE\n",
    "    LEFT JOIN flatline_execute_flags fl\n",
    "        ON COALESCE(sl.station, vp.node) = fl.station\n",
    "        AND m.cpt_time_local = fl.cpt_time_local\n",
    ")\n",
    "SELECT\n",
    "    *,\n",
    "    -- Derived flags (comma-separated)\n",
    "    ARRAY_TO_STRING(LIST_VALUE(\n",
    "        CASE WHEN CAST(bucket_upper AS FLOAT) <= 10 THEN 'risk-under' END,\n",
    "        CASE WHEN CAST(flatline_execute AS INT) = 1 THEN 'flatline' END\n",
    "    ).list_filter(x -> x IS NOT NULL), ',') AS flags\n",
    "FROM base\n",
    "ORDER BY station, cpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name: summary | type: python\n",
    "# Summary of available_inputs breakdown\n",
    "summary = conn.execute(\"\"\"\n",
    "    SELECT available_inputs, COUNT(DISTINCT station) AS stations, COUNT(*) AS rows\n",
    "    FROM joined\n",
    "    GROUP BY available_inputs\n",
    "    ORDER BY available_inputs\n",
    "\"\"\").fetchdf()\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "total = conn.execute(\"SELECT COUNT(*) FROM joined\").fetchone()[0]\n",
    "distinct = conn.execute(\"SELECT COUNT(DISTINCT station) FROM joined\").fetchone()[0]\n",
    "print(f'\\nTotal: {distinct} stations, {total} rows')\n",
    "\n",
    "result = {\n",
    "    'status': 'success',\n",
    "    'total_rows': total,\n",
    "    'distinct_stations': distinct,\n",
    "    'breakdown': summary.to_dict('records')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name: save_joined | type: python\n",
    "# Export joined table to CSV\n",
    "output_file = ctx_dir / 'joined.csv'\n",
    "conn.execute(f\"COPY joined TO '{output_file}' (HEADER, DELIMITER ',')\")\n",
    "row_count = conn.execute(\"SELECT COUNT(*) FROM joined\").fetchone()[0]\n",
    "print(f'Saved joined table: {row_count} rows -> {output_file}')\n",
    "\n",
    "result = {'output_file': str(output_file), 'rows': row_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- name: pba_visual | type: sql\n",
    "SET TimeZone = 'UTC';\n",
    "\n",
    "CREATE OR REPLACE TABLE pba_visual AS\n",
    "WITH vovi_prepared AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        CAST(timezone('UTC', to_timestamp(station_cpt)) AS TIMESTAMP) AS cpt_utc\n",
    "    FROM vovi\n",
    "),\n",
    "vp_keys AS (\n",
    "    SELECT DISTINCT station, cpts_utc\n",
    "    FROM joined\n",
    "    WHERE cpts_utc IS NOT NULL\n",
    "),\n",
    "vp_target_date AS (\n",
    "    SELECT MAX(target_forecast_date::DATE) AS target_date FROM cumulative\n",
    "),\n",
    "\n",
    "-- Target date: cumulative data (has fan chart percentiles)\n",
    "target_rows AS (\n",
    "    SELECT\n",
    "        'target' AS pba_type,\n",
    "        (SELECT target_date FROM vp_target_date) || '#' || C.station || '#' || strftime(C.cpt_utc::TIMESTAMP, '%H:%M') AS grid_key,\n",
    "        C.target_forecast_date::VARCHAR AS pba_ofd_date,\n",
    "        C.dhm_horizon AS pba_dhm_horizon,\n",
    "        C.bi_hourly_trunc_local::VARCHAR AS pba_bi_hourly_local,\n",
    "        C.horizon_rank_local::FLOAT AS pba_horizon_rank,\n",
    "        C.scheduled::FLOAT AS pba_scheduled,\n",
    "        COALESCE(C.soft_cap::FLOAT, C.target_horizon_soft_cap::FLOAT, 0) AS pba_soft_cap,\n",
    "        I.hard_cap::FLOAT AS pba_hard_cap,\n",
    "        COALESCE(C.slammed::FLOAT, 0) AS pba_slammed,\n",
    "        I.cap_utilization::FLOAT AS pba_cap_utilization,\n",
    "        COALESCE(C.cumulative_median::FLOAT, 0) AS pba_cumulative_median,\n",
    "        COALESCE(C.cumulative_median_adj::FLOAT, 0) AS pba_cumulative_median_adj,\n",
    "        COALESCE(C.cumulative_p10::FLOAT, 0) AS pba_p10,\n",
    "        COALESCE(C.cumulative_p30::FLOAT, 0) AS pba_p30,\n",
    "        COALESCE(C.cumulative_p50::FLOAT, 0) AS pba_p50,\n",
    "        COALESCE(C.cumulative_p70::FLOAT, 0) AS pba_p70,\n",
    "        COALESCE(C.cumulative_p90::FLOAT, 0) AS pba_p90\n",
    "    FROM cumulative C\n",
    "    INNER JOIN vp_keys K\n",
    "        ON C.station = K.station\n",
    "        AND strftime(C.cpt_utc::TIMESTAMP, '%H:%M') = K.cpts_utc\n",
    "    LEFT JOIN intraday_pba I\n",
    "        ON C.station = I.station\n",
    "        AND C.cpt_utc::TIMESTAMP = I.cpt_utc\n",
    "        AND C.target_forecast_date::DATE = I.ofd_date\n",
    "        AND C.horizon_rank_local::FLOAT = I.horizon_rank::FLOAT\n",
    "    WHERE C.horizon_day > -2 OR (C.horizon_day = -2 AND C.horizon_hour >= 12)\n",
    "),\n",
    "\n",
    "-- Match date: intraday_pba via VOVI match_key (no fan chart data)\n",
    "match_rows AS (\n",
    "    SELECT\n",
    "        'match' AS pba_type,\n",
    "        (SELECT target_date FROM vp_target_date) || '#' || I.station || '#' || strftime(I.cpt_utc, '%H:%M') AS grid_key,\n",
    "        I.ofd_date::VARCHAR AS pba_ofd_date,\n",
    "        I.dhm_horizon AS pba_dhm_horizon,\n",
    "        I.bi_hourly_trunc_local::VARCHAR AS pba_bi_hourly_local,\n",
    "        I.horizon_rank::FLOAT AS pba_horizon_rank,\n",
    "        COALESCE(I.scheduled::FLOAT, 0) AS pba_scheduled,\n",
    "        COALESCE(I.soft_cap::FLOAT, 0) AS pba_soft_cap,\n",
    "        COALESCE(I.hard_cap::FLOAT, 0) AS pba_hard_cap,\n",
    "        COALESCE(I.slammed::FLOAT, 0) AS pba_slammed,\n",
    "        COALESCE(I.cap_utilization::FLOAT, 0) AS pba_cap_utilization,\n",
    "        NULL::FLOAT AS pba_cumulative_median,\n",
    "        NULL::FLOAT AS pba_cumulative_median_adj,\n",
    "        NULL::FLOAT AS pba_p10,\n",
    "        NULL::FLOAT AS pba_p30,\n",
    "        NULL::FLOAT AS pba_p50,\n",
    "        NULL::FLOAT AS pba_p70,\n",
    "        NULL::FLOAT AS pba_p90\n",
    "    FROM vp_keys K\n",
    "    INNER JOIN vovi_prepared V\n",
    "        ON K.station = V.station\n",
    "        AND K.cpts_utc = strftime(V.cpt_utc, '%H:%M')\n",
    "    INNER JOIN intraday_pba I\n",
    "        ON V.station = I.station\n",
    "        AND strftime(V.cpt_utc, '%H:%M') = strftime(I.cpt_utc, '%H:%M')\n",
    "        AND V.match_key IS NOT NULL\n",
    "        AND I.ofd_date = V.match_key::DATE\n",
    "    WHERE I.horizon_day > -2 OR (I.horizon_day = -2 AND I.horizon_hour >= 12)\n",
    ")\n",
    "\n",
    "SELECT * FROM target_rows\n",
    "UNION ALL\n",
    "SELECT * FROM match_rows\n",
    "ORDER BY grid_key, pba_type, pba_horizon_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name: save_pba_visual | type: python\n",
    "# Export PBA visual data to JSON for frontend chart consumption\n",
    "import json as _json\n",
    "import math as _math\n",
    "\n",
    "pba_df = conn.execute(\"SELECT * FROM pba_visual\").fetchdf()\n",
    "output_file = ctx_dir / 'visual.json'\n",
    "\n",
    "# Convert to list of dicts, replacing NaN with None for valid JSON\n",
    "records = pba_df.to_dict('records')\n",
    "for rec in records:\n",
    "    for k, v in rec.items():\n",
    "        if isinstance(v, float) and _math.isnan(v):\n",
    "            rec[k] = None\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    _json.dump(records, f, default=str)\n",
    "\n",
    "row_count = len(records)\n",
    "distinct_keys = pba_df['grid_key'].nunique()\n",
    "target_count = len(pba_df[pba_df['pba_type'] == 'target'])\n",
    "match_count = len(pba_df[pba_df['pba_type'] == 'match'])\n",
    "print(f'Saved PBA visual data: {row_count} rows ({target_count} target, {match_count} match), {distinct_keys} grid_keys -> {output_file}')\n",
    "\n",
    "result = {'output_file': str(output_file), 'rows': row_count, 'target': target_count, 'match': match_count, 'distinct_grid_keys': distinct_keys}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "etl": {
   "name": "Forecast Setup (Timezone)",
   "description": "Timezone-parameterized pipeline. Fetches CT metadata, filters by timezone bucket, pulls VP/VOVI/S3 artifacts.",
   "variables": {
    "tz_bucket": {
     "type": "text",
     "label": "Timezone Bucket",
     "default": "Eastern"
    },
    "biz": {
     "type": "text",
     "label": "Business Line",
     "default": "AMZL"
    },
    "aws_profile": {
     "type": "text",
     "label": "AWS Profile for S3",
     "default": "forecast-artifacts"
    }
   },
   "sources": [],
   "outputs": [],
   "options": {
    "return_preview": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}